{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juKfq-gyF2MM"
   },
   "source": [
    "# Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH7QiG34DScj"
   },
   "source": [
    "**Texto a analisar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1614301096545,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "BkgBPODWDEgo"
   },
   "outputs": [],
   "source": [
    "# Notar o uso do apóstrofo\n",
    "texto = \"O rato roeu a roupa do Rei de Roma. Em seguida, jogou-se n'água do Mar Tirreno... Calcula-se que o prejuízo tenha sido alto: R$ 10.000,00.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Azerl_5kL8hA"
   },
   "source": [
    "**Manual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1614301208248,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "zmP2WwCzFLdO",
    "outputId": "ff60c90e-1826-4786-d619-5f0dcccee74d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'Rei', 'de', 'Roma.', 'Em', 'seguida,', 'jogou-se', \"n'água\", 'do', 'Mar', 'Tirreno...', 'Calcula-se', 'que', 'o', 'prejuízo', 'tenha', 'sido', 'alto:', 'R$', '10.000,00.']\n"
     ]
    }
   ],
   "source": [
    "tokens = texto.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArUMzX45L_08"
   },
   "source": [
    "**NLTK**\n",
    "\n",
    "Vantagens: \n",
    "\n",
    "*   É possível tokenizar não só palavras, mas também sentenças.\n",
    "*   O tokenizador considera especificidades linguísticas, como apóstrofos e hífens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1614301442656,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "2JBtCZ3xXN9g",
    "outputId": "738d0da0-0ffc-482d-b67c-c36e95aa5d41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/brunorosilva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importação do módulo e dos recursos necessários\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1614301467889,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "z31DuxRqMisb",
    "outputId": "f02429f0-23c8-4c3a-9a37-b4c185778fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'Rei', 'de', 'Roma', '.', 'Em', 'seguida', ',', 'jogou-se', \"n'água\", 'do', 'Mar', 'Tirreno', '...', 'Calcula-se', 'que', 'o', 'prejuízo', 'tenha', 'sido', 'alto', ':', 'R', '$', '10.000,00', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização de palavras e pontuação\n",
    "from nltk import tokenize    \n",
    "tokens = tokenize.word_tokenize(texto, language='portuguese')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1614301623449,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "JXpqzZb3WV01",
    "outputId": "1139a8a7-8102-46ad-c324-37d6cf834f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O rato roeu a roupa do Rei de Roma.', \"Em seguida, jogou-se n'água do Mar Tirreno...\", 'Calcula-se que o prejuízo tenha sido alto: R$ 10.000,00.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização de sentenças\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "sents = sent_tokenizer.tokenize(texto)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXKbthKCMri_"
   },
   "source": [
    "**spaCy** \n",
    "\n",
    "Vantagem:\n",
    "\n",
    "*   É o que mais bem funciona em geral.\n",
    "\n",
    "Desvantagens:\n",
    "\n",
    "\n",
    "*   Os modelos têm de estar pré-instalados;\n",
    "*   O carregamento dos modelos na memória é lento;\n",
    "*   Ocupa mais memória que o NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14748,
     "status": "ok",
     "timestamp": 1614301740368,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "DeTSAgE6SLpA",
    "outputId": "a1cdc5f3-2b56-42e4-ec3b-0422979314e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in /home/brunorosilva/.local/lib/python3.8/site-packages (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (8.0.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (4.56.0)\n",
      "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "2021-03-10 18:19:08.116957: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2021-03-10 18:19:08.117006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Requirement already satisfied: pt-core-news-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.0.0/pt_core_news_sm-3.0.0-py3-none-any.whl#egg=pt_core_news_sm==3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from pt-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (45.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.18.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (4.56.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.22.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (1.1.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pt-core-news-sm==3.0.0) (7.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# Instalação/atualização do spaCy e dos modelos de língua\n",
    "!pip3 install -U spacy\n",
    "\n",
    "!python3 -m spacy download pt_core_news_sm  # Notar o \"sm\": Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9534,
     "status": "ok",
     "timestamp": 1614301765660,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "z9amHijGZrpo",
    "outputId": "074ff9ed-e63d-4dca-c3bb-26c93693dbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-10 18:19:19.920686: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2021-03-10 18:19:19.920723: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.0.5                         \n",
      "Location         /home/brunorosilva/.local/lib/python3.8/site-packages/spacy\n",
      "Platform         Linux-5.8.0-44-generic-x86_64-with-glibc2.29\n",
      "Python version   3.8.5                         \n",
      "Pipelines        pt_core_news_lg (3.0.0), pt_core_news_sm (3.0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importação do módulo e do modelo de língua\n",
    "import spacy\n",
    "!python3 -m spacy info  # P/ checar a versão (tem de ser >= 3.0)\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1614301779156,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "7M39buVGYyaW",
    "outputId": "debb7576-531c-46f9-b181-75e0a6ab5025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'rato', 'roeu', 'a', 'roupa', 'do', 'Rei', 'de', 'Roma', '.', 'Em', 'seguida', ',', 'jogou-se', \"n'água\", 'do', 'Mar', 'Tirreno', '...', 'Calcula-se', 'que', 'o', 'prejuízo', 'tenha', 'sido', 'alto', ':', 'R$', '10.000,00', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenização de palavras e pontuação\n",
    "def tokenizar(string):\n",
    "    doc = nlp(string)\n",
    "    return [t.text for t in doc]  #  Lista de atributos .text de cada token\n",
    "\n",
    "tokens = tokenizar(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1614301836209,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "VXTR4CdJbonp",
    "outputId": "81811512-0c58-493d-e498-6031ddb3d478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O rato roeu a roupa do Rei de Roma.\n",
      "Em seguida, jogou-se n'água do Mar Tirreno...\n",
      "Calcula-se que o prejuízo tenha sido alto: R$ 10.000,00.\n"
     ]
    }
   ],
   "source": [
    "# Tokenização de sentenças\n",
    "def sents(string):\n",
    "    doc = nlp(string)\n",
    "    return doc.sents \n",
    "\n",
    "for s in sents(texto):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVyFdanSwMsZ"
   },
   "source": [
    "# Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1614281006211,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "KRenTNimHLlj"
   },
   "outputs": [],
   "source": [
    "def limpar(lista):\n",
    "    return [i.lower() for i in lista if i.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUYuS-ufKol7"
   },
   "source": [
    "# Palavras vazias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1YcYD3pLRf7"
   },
   "source": [
    "**NLTK**\n",
    "\n",
    "*   Uma lista pequena, mas rápida de se carregar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gvPHilVddo39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/brunorosilva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "len(stops_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz4xtCmVLhbW"
   },
   "source": [
    "**spaCy**\n",
    "\n",
    "\n",
    "*   Uma lista mais abrangente, mas mais lenta e com mais exigência de memória para carregar o modelo.\n",
    "* Atenção! A \"lista\" do spaCy é, na verdade, um conjunto (set).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5FQo5LltLQCP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "len(stops_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb_jpIGhevpE"
   },
   "source": [
    "**União das duas listas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JSShweY3fHxY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = set(stops_nltk) | stops_spacy\n",
    "\n",
    "len(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsfw4i7YKxuL"
   },
   "source": [
    "# Stemização\n",
    "Exclusivamente no NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1614302367682,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "D1pPnArUUByF",
    "outputId": "105c4d42-622a-4e2b-87e3-afc8369218ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nad\n",
      "nad\n",
      "nadabób\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     /home/brunorosilva/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Carregamento do stemizador\n",
    "nltk.download('rslp')\n",
    "raiz = nltk.stem.RSLPStemmer().stem\n",
    "\n",
    "print(raiz('nada'))\n",
    "print(raiz('nadinha'))\n",
    "print(raiz('nadabóbora'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmyB7QdAK4Kg"
   },
   "source": [
    "# Lematização\n",
    "\n",
    "\n",
    "*   Exclusivamente no spaCy\n",
    "*   O resultado tem muitos erros mas, ao usar o modelo grande (large), a performance melhora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24228,
     "status": "ok",
     "timestamp": 1614302840075,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "qH7EefolMmPy",
    "outputId": "f517cf81-1559-4836-98ab-42f58c1728ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-10 18:19:27.593359: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2021-03-10 18:19:27.593399: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Requirement already satisfied: pt-core-news-lg==3.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.0.0/pt_core_news_lg-3.0.0-py3-none-any.whl#egg=pt_core_news_lg==3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from pt-core-news-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (20.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (45.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.18.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (4.56.0)\n",
      "Requirement already satisfied: jinja2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (8.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/brunorosilva/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.14.0)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /home/brunorosilva/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (1.1.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/brunorosilva/.local/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->pt-core-news-lg==3.0.0) (7.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "# Baixando o modelo de língua grande (lg) para o português\n",
    "!python3 -m spacy download pt_core_news_lg\n",
    "\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 975,
     "status": "ok",
     "timestamp": 1614302851438,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "egI2HTZ4Og7T",
    "outputId": "ab57ed4e-e1e9-4f30-d283-b90dbd6a46e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Se', 'este', 'palavra', 'ser', 'lematizar', ',', 'ficar', 'muito', 'diferente', '.']\n",
      "['Seu', 'memorar', 'ser', 'umar', 'pancada', 'de', 'ferrar', 'que', 'não', 'valer', 'nado', '.']\n"
     ]
    }
   ],
   "source": [
    "def lema(string):\n",
    "    doc = nlp(string)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "print(lema('Se estas palavras fossem lematizadas, ficariam muitíssimo diferentes.'))\n",
    "print(lema('Seu memorando é uma pancada de ferro que não vale nada.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdxWgbfpKi-V"
   },
   "source": [
    "# Hápax Legômena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1614281249235,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "AUfLzIlJ4-p-"
   },
   "outputs": [],
   "source": [
    "# Para criar uma lista de hápax legômena\n",
    "def hapax(lista):\n",
    "    return [i for i in lista if lista.count(i) == 1]\n",
    "\n",
    "# Para eliminá-los da lista de tokens\n",
    "def sem_hapax(lista):\n",
    "    return [i for i in lista if lista.count(i) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUj-THKQkkql"
   },
   "source": [
    "# Mãos à obra!\n",
    "Vamos trabalhar com a leitura de um arquivo de texto bruto e aplicação do pipeline completo de pré-processamento dos dados e a contagem das unidades lexicais que vimos até agora. Usaremos o livro \"O Guarani\", de José de Alencar.\n",
    "\n",
    "**Exercícios**\n",
    "\n",
    "1. Abra o arquivo \"Guarani.txt\" e calcule:\n",
    "\n",
    "*   O número total de caracteres\n",
    "*   O número de caracteres sem espaços em branco\n",
    "*   O número de palavras (dica: para um trabalho de boa qualidade, tokenize e limpe o texto)\n",
    "*   A riqueza lexical\n",
    "\n",
    "\n",
    "2. Mostre as 20 palavras mais frequentes do texto em ordem descrescente de ocorrência.\n",
    "\n",
    "3. Discuta o resultado. Qual a característica mais evidente dessas palavras frequentes?\n",
    "\n",
    "4. Calcule:\n",
    "*   O vocabulário do livro (o número de lemas)\n",
    "*   A riqueza vocabular. Compare com a riqueza lexical.\n",
    "\n",
    "5. Reduza os tokens a raízes.\n",
    "* O conjunto das raízes é maior ou menor que o vocabulário (de lemas)?\n",
    "* Discuta as vantagens e os riscos implicados no uso de um ou outro.\n",
    "\n",
    "\n",
    "6. Crie uma lista dos hápax legômena e responda: \n",
    "* Qual a proporção de hápax legômena no vocabulário?\n",
    "* Exiba os 50 primeiros hápax legômena da sua lista. Baseando-se no que você vê, será que existem traços em comum entre eles?  Discuta se poderiam ser eliminados sem prejuízo para uma análise do conteúdo do texto.\n",
    "\n",
    "7. Crie uma versão do livro \"privilegiando o conteúdo\": uma lista sem hápax legômena, sem palavras vazias e com as palavras restantes stemizadas. \n",
    "* Compare o número de tokens desse \"livro\" com o primeiro cálculo de tokens, aquele sobre o texto original. \n",
    "* Leia o resultado. Que tal?...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89649,
     "status": "ok",
     "timestamp": 1614281415717,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "F_UBquo-tlbm",
    "outputId": "52620a18-ecd6-4588-f8cf-30b028fc133a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteres:  626296\n",
      "Caracteres sem espaços:  523030\n",
      "Palavras:  103688\n",
      "Riqueza lexical:  0.10305917753259779\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "#Exercício 1\n",
    "arq = open('Guarani.txt', 'r')\n",
    "texto = arq.read()\n",
    "arq.close()\n",
    "\n",
    "print('Caracteres: ', len(texto))\n",
    "print('Caracteres sem espaços: ', len(texto) - texto.count(' '))\n",
    "tokens = limpar(tokenizar(texto))\n",
    "print('Palavras: ', len(tokens))\n",
    "print('Riqueza lexical: ', len(set(tokens)) / len(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 767,
     "status": "ok",
     "timestamp": 1614281508593,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "ROqLm3FgqVrh",
    "outputId": "0eee77db-278e-4232-9d04-d05d6484a7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('a', 4392)\n",
      "1 ('que', 4283)\n",
      "2 ('o', 3899)\n",
      "3 ('de', 3867)\n",
      "4 ('e', 3587)\n",
      "5 ('um', 1653)\n",
      "6 ('do', 1367)\n",
      "7 ('não', 1276)\n",
      "8 ('uma', 1183)\n",
      "9 ('se', 1090)\n",
      "10 ('da', 1087)\n",
      "11 ('os', 1045)\n",
      "12 ('com', 1011)\n",
      "13 ('sua', 917)\n",
      "14 ('para', 849)\n",
      "15 ('seu', 771)\n",
      "16 ('peri', 728)\n",
      "17 ('em', 698)\n",
      "18 ('as', 697)\n",
      "19 ('por', 655)\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Exercício 2\n",
    "from collections import Counter\n",
    "c = Counter(tokens)\n",
    "for e, i in enumerate(c.most_common(20)):\n",
    "    print(e, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60359,
     "status": "ok",
     "timestamp": 1614281638709,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "B1QD-tY0qoow",
    "outputId": "7db1cab0-4e91-4fcb-c81b-15719e201902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário:  5313  palavras.\n",
      "Riqueza vocabular:  0.05124025923925623\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Exercício 4\n",
    "lemas = lema(' '.join(tokens))\n",
    "vocab = set(lemas)\n",
    "print('Vocabulário: ', len(vocab), ' palavras.')\n",
    "print('Riqueza vocabular: ', len(vocab) / len(lemas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1614281926946,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "bUQXn-29qzRv",
    "outputId": "ce898f30-7b7e-417c-b615-6052b9591782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinalidade do conjunto de raízes:  4665\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Exercício 5\n",
    "raizes = [raiz(t) for t in tokens]\n",
    "print('Cardinalidade do conjunto de raízes: ', len(set(raizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175489,
     "status": "ok",
     "timestamp": 1614281839289,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "cpuMX-rCrnwh",
    "outputId": "37393833-e24f-4506-a03d-baacd114a992"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c08a488c7e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Exercício 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhapax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Os hápax legômena ocupam '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' do conjunto total de tokens.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-91edb7397ae2>\u001b[0m in \u001b[0;36mhapax\u001b[0;34m(lista)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Para criar uma lista de hápax legômena\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhapax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlista\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Para eliminá-los da lista de tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-91edb7397ae2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Para criar uma lista de hápax legômena\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhapax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlista\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Para eliminá-los da lista de tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Exercício 6\n",
    "hap = hapax(tokens)\n",
    "print('Os hápax legômena ocupam ', len(hap)/len(set(tokens)), ' do conjunto total de tokens.')\n",
    "print(hap[:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46364,
     "status": "ok",
     "timestamp": 1614282320719,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "6rRNo5zZrzOq",
    "outputId": "0c446540-c5f3-4a2e-9105-ee36e73db753"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Exercício 7\n",
    "tokens_plenos = [t for t in tokens if t not in stops]\n",
    "tokens_plenos_2 = sem_hapax(tokens_plenos)\n",
    "raiz_tokens_plenos_2 = [raiz(t) for t in tokens_plenos_2]\n",
    "print('Raízes sem stopwords e sem hápax legômena: ', len(raiz_tokens_plenos_2))\n",
    "print(' '.join(raiz_tokens_plenos_2[1000:1050]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8xSIQe_scqh"
   },
   "source": [
    "**Bônus: Geração de nuvens de palavras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1614282454191,
     "user": {
      "displayName": "Marcos Lopes",
      "photoUrl": "",
      "userId": "07927712262573563042"
     },
     "user_tz": 180
    },
    "id": "FCghYf8dsW54",
    "outputId": "d3769bb5-248f-4c04-a21e-89ec946367c4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nuvem = WordCloud(stopwords=stops,\n",
    "                    background_color='black',\n",
    "                    width=1600, height=800).generate(texto)\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.imshow(nuvem, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "plt.imshow(nuvem);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNmPVSrkiLvcDyN2v3uVEFV",
   "collapsed_sections": [],
   "name": "Pré-Processamento.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
