{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sxdL1QNNGTM"
   },
   "source": [
    "# Aula 7\n",
    "\n",
    "# \"Data Pipelines\" - Parte 3\n",
    "\n",
    "# Dados estruturados usando `feature_column`\n",
    "\n",
    "### Eduardo Lobo Lustosa Cabral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfyjJeLcNGTN"
   },
   "source": [
    "## 1. Objetivos\n",
    "\n",
    "Apresentar o módulo `feature_column` do TensorFlow para realizar processamento de dados estruturados.\n",
    "\n",
    "Apresentar a camada tipo **`DenseFeatures`** do Keras, que permite incluir o processamento dos dados dentro da rede.\n",
    "\n",
    "Exemplos de \"data pipeline\" completo para dados estruturados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUfH9fPVEeF8"
   },
   "source": [
    "### Importa principais biblitecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dUACLoa5EeF8"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqJN2uQEEeF9"
   },
   "source": [
    "## 2. Transformação de dados com o TensorFlow\n",
    "\n",
    "Com vimos, para processar e carregar dados de forma eficiente para treinar e utilizar uma RNA deve-se criar um \"data pipeline\". \n",
    "\n",
    "Os **Data pipelines** realizam três etapas de operação com os dados (Extrair, Transformar e Carregar):\n",
    "\n",
    "- **Extrair** $\\to$ carrega dados originais do local onde se encontram e traz para o nosso ambiente de computação;\n",
    "- **Transformar** $\\to$ processa os dados para serem colocados em formatos adequados que podem ser usados por uma RNA;\n",
    "- **Carregar** $\\to$ alimenta a RNA com dados durante o seu treinamento ou para realizar previsões.\n",
    "\n",
    "O pré-processamento de dados (transformação) durante a fase de desenvolvimento de uma nova aplicação é realizado pelo desenvolvedor e pode ser realizado separadamente do treinamento da rede. Porém, na etapa de utilização da rede por outros usuários, o pré-processamento dos dados deve ser realizado de forma automática. Assim, a melhor forma é incluir o pré-processamento dentro da rede.\n",
    "\n",
    "Como no Keras qualquer operação realizada por uma RNA deve ser uma camada, o módulo **`feature_column`** do TensorFlow permite incluir camadas em uma RNA para transformar dados (https://www.tensorflow.org/api_docs/python/tf/feature_column).\n",
    "\n",
    "O módulo `feature_column` do TensorFlow serve para realizar transformações de dados estruturados $\\to$ pode-se fazer praticamente qualquer tipo de transformação usando o módulo `feature_column`. \n",
    "\n",
    "Vamos ver algumas funcionalidades desse módulo para processar dados estruturados. \n",
    "\n",
    "O processamento de dados usado o módulo `feature_column` tem muitas vantagens em relação a usar outras forma, tais como, Pandas ou o Numpy. Entre essas vantagens tem-se:\n",
    "\n",
    "1. Existem muitas ferramentas disponíveis;\n",
    "\n",
    "\n",
    "2. Os dados processados podem ser facilmente transformados em um objeto `Dataset` do TensorFlow;\n",
    "\n",
    "\n",
    "3. Como já visto, os objetos `Dataset` foram desenvolvidos para trabalhar em conjunto com o método `fit` do Keras e tem a capacidade de tornar o treinamento muito mais rápido em razão de otimizar o carregamento de lotes de dados na CPU/GPU.\n",
    "\n",
    "\n",
    "4. Permite realizar em conjunto com uma camada tipo **DenseFeatrures** (https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures) a transformação dos dados dentro da rede, facilitando o seu uso por outros usuários.\n",
    "\n",
    "A seguir veremos como realizar algumas dessas transformações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcStOLvRJHVV"
   },
   "source": [
    "## 3. Transformações em dados estruturados e textos\n",
    "\n",
    "Para dados estruturados, onde cada característica consiste de uma coluna do conjunto de dados, as transformações são realizadas nas colunas.\n",
    "\n",
    "As principais transformações realizadas em dados estruturados e textos são as seguintes:\n",
    "\n",
    "- Colunas numéricas;\n",
    "- Segmentação\n",
    "- Categorias;\n",
    "- Codificação \"one-hot\";\n",
    "- Codificação \"Hashed\";\n",
    "- Codificação \"embedding\";\n",
    "- Cruzamento de colunas.\n",
    "\n",
    "Na Aula 3 vimos o que significam quase todas essas transformações e como fazer usando o Pandas e Numpy.\n",
    "\n",
    "O módulo `feature_column` do TensorFlow realiza todas essas transformações e muitas outras. A lista completa de transformações disponíveis pode ser vista em https://www.tensorflow.org/api_docs/python/tf/feature_column.\n",
    "\n",
    "`Feature_columns` são objetos usados para alimentar RNAs de forma eficiente e quando é usado em dados estruturados todas as colunas de dados devem ser transformadas, não importando se forem dados numéricos ou não."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXIIq4ih3aFy"
   },
   "source": [
    "### 3.1 Conjunto de dados\n",
    "\n",
    "Vamos utilizar um sub-conjunto dos dados do conjunto de dados **Wine Reviews**, disponível no Kaggle (https://www.kaggle.com/christopheiv/winemagdata130k). Esse conjunto de dados completo consiste de 130 mil avaliações de vinhos, que inclui variedade, região onde é produzido, vinícola, preço e descrição do vinho.\n",
    "\n",
    "Para exemplificar as transformações realizadas com o módulo `feature_column`, vamos utilizar 1001 linhas desse conjunto de dados e as seguintes características:\n",
    "\n",
    "- \"country\";\n",
    "- \"description\";\n",
    "- \"points\";\n",
    "- \"price\";\n",
    "- \"province\";\n",
    "- \"variety\".\n",
    "\n",
    "Esses dados estão em um arquivo tipo CSV. Cada linha representa um vinho e cada coluna descreve uma característica do vinho. O objetivo desses dados é prever o preço do vinho, assim, consiste de um problema de regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ2_le6sEeF-"
   },
   "source": [
    "####  Carregar conjunto de dados\n",
    "\n",
    "Vamos usar o Pandas para carregar os dados de um arquivo CSV e carregá-lo em um objeto Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "-wkRDn8eEeF-",
    "outputId": "4057bd48-38a7-458c-b309-2f684dd1f854"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimston...</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>White Blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Portuguese Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Riesling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Here's a Syrah with bursting aromas of mature ...</td>\n",
       "      <td>88</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Syrah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Blended from a patchwork of old vineyards thro...</td>\n",
       "      <td>88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Shiraz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>US</td>\n",
       "      <td>Rich in the mouth, this creamy and textural wi...</td>\n",
       "      <td>88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>US</td>\n",
       "      <td>Creamy and textural, this brings on a nice mix...</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Riesling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>US</td>\n",
       "      <td>Arcane's Cab is stylistically apart from eithe...</td>\n",
       "      <td>88</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1001 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                                        description  points  \\\n",
       "0         Italy  Aromas include tropical fruit, broom, brimston...      87   \n",
       "1      Portugal  This is ripe and fruity, a wine that is smooth...      87   \n",
       "2            US  Tart and snappy, the flavors of lime flesh and...      87   \n",
       "3            US  Pineapple rind, lemon pith and orange blossom ...      87   \n",
       "4            US  Much like the regular bottling from 2012, this...      87   \n",
       "...         ...                                                ...     ...   \n",
       "996       Italy  Here's a Syrah with bursting aromas of mature ...      88   \n",
       "997   Australia  Blended from a patchwork of old vineyards thro...      88   \n",
       "998          US  Rich in the mouth, this creamy and textural wi...      88   \n",
       "999          US  Creamy and textural, this brings on a nice mix...      88   \n",
       "1000         US  Arcane's Cab is stylistically apart from eithe...      88   \n",
       "\n",
       "      price           province             variety  \n",
       "0       NaN  Sicily & Sardinia         White Blend  \n",
       "1      15.0              Douro      Portuguese Red  \n",
       "2      14.0             Oregon          Pinot Gris  \n",
       "3      13.0           Michigan            Riesling  \n",
       "4      65.0             Oregon          Pinot Noir  \n",
       "...     ...                ...                 ...  \n",
       "996    14.0  Sicily & Sardinia               Syrah  \n",
       "997    18.0    South Australia              Shiraz  \n",
       "998    18.0             Oregon          Pinot Gris  \n",
       "999    17.0             Oregon            Riesling  \n",
       "1000   24.0             Oregon  Cabernet Sauvignon  \n",
       "\n",
       "[1001 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega dados em um DataFrame Pandas\n",
    "df = pd.read_csv('wine_review_small.csv', index_col=0)\n",
    "\n",
    "# Apresenta DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYeYwtMBEeF_"
   },
   "source": [
    "- Observa-se que existem dados de 1001 vinhos, ou seja, existem 1001 exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fikaH5jEeGA"
   },
   "source": [
    "Vamos verificar o tipo de dados e calcular as principais estatísticas dos dados de cada coluna do DataFrame para entender um pouco como são esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1001 entries, 0 to 1000\n",
      "Data columns (total 6 columns):\n",
      "country        1000 non-null object\n",
      "description    1001 non-null object\n",
      "points         1001 non-null int64\n",
      "price          944 non-null float64\n",
      "province       1000 non-null object\n",
      "variety        1001 non-null object\n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 54.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Tipos de dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "iUY5EamPEeGB",
    "outputId": "b37eeaff-f2a4-4eab-ab5a-ab15eff63923"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1001.000000</td>\n",
       "      <td>944.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>88.581419</td>\n",
       "      <td>37.337924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.625569</td>\n",
       "      <td>47.302631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>43.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>775.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            points       price\n",
       "count  1001.000000  944.000000\n",
       "mean     88.581419   37.337924\n",
       "std       2.625569   47.302631\n",
       "min      80.000000    7.000000\n",
       "25%      87.000000   17.000000\n",
       "50%      88.000000   27.000000\n",
       "75%      90.000000   43.250000\n",
       "max     100.000000  775.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Principais estatísticas das colunas numéricas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificar colunas que possuem dados não existentes\n",
    "\n",
    "A primeira etapa do processamento de dados é verificar se existem dados ausentes (NaN) e onde eles estão.\n",
    "\n",
    "Para isso podemos criar uma função que procura dados ausentes em uma coluna, usando o método `isnull()`, e depois utilizá-la com o método `apply()` para procurar em todas as colunas. \n",
    "\n",
    "O código da célula abaixo realiza essa operação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores ausentes por coluna\n",
      "country         1\n",
      "description     0\n",
      "points          0\n",
      "price          57\n",
      "province        1\n",
      "variety         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cria função que calcula número de NaNs em uma coluna\n",
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "# Aplica função num_missing em todas as colunas\n",
    "print(\"Valores ausentes por coluna\")\n",
    "print(df.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correção de erros\n",
    "\n",
    "Vamos remover todas as linhas que possuem dados faltantes (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[pd.isnull(df.country)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[pd.isnull(df.province)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>Portuguese Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and...</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom ...</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Riesling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this...</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Noir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Blackberry and raspberry aromas show a typical...</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Tempranillo-Merlot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Here's a Syrah with bursting aromas of mature ...</td>\n",
       "      <td>88</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Syrah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Blended from a patchwork of old vineyards thro...</td>\n",
       "      <td>88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>Shiraz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>US</td>\n",
       "      <td>Rich in the mouth, this creamy and textural wi...</td>\n",
       "      <td>88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Pinot Gris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>US</td>\n",
       "      <td>Creamy and textural, this brings on a nice mix...</td>\n",
       "      <td>88</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Riesling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>US</td>\n",
       "      <td>Arcane's Cab is stylistically apart from eithe...</td>\n",
       "      <td>88</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                                        description  points  \\\n",
       "1      Portugal  This is ripe and fruity, a wine that is smooth...      87   \n",
       "2            US  Tart and snappy, the flavors of lime flesh and...      87   \n",
       "3            US  Pineapple rind, lemon pith and orange blossom ...      87   \n",
       "4            US  Much like the regular bottling from 2012, this...      87   \n",
       "5         Spain  Blackberry and raspberry aromas show a typical...      87   \n",
       "...         ...                                                ...     ...   \n",
       "996       Italy  Here's a Syrah with bursting aromas of mature ...      88   \n",
       "997   Australia  Blended from a patchwork of old vineyards thro...      88   \n",
       "998          US  Rich in the mouth, this creamy and textural wi...      88   \n",
       "999          US  Creamy and textural, this brings on a nice mix...      88   \n",
       "1000         US  Arcane's Cab is stylistically apart from eithe...      88   \n",
       "\n",
       "      price           province             variety  \n",
       "1      15.0              Douro      Portuguese Red  \n",
       "2      14.0             Oregon          Pinot Gris  \n",
       "3      13.0           Michigan            Riesling  \n",
       "4      65.0             Oregon          Pinot Noir  \n",
       "5      15.0     Northern Spain  Tempranillo-Merlot  \n",
       "...     ...                ...                 ...  \n",
       "996    14.0  Sicily & Sardinia               Syrah  \n",
       "997    18.0    South Australia              Shiraz  \n",
       "998    18.0             Oregon          Pinot Gris  \n",
       "999    17.0             Oregon            Riesling  \n",
       "1000   24.0             Oregon  Cabernet Sauvignon  \n",
       "\n",
       "[943 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[pd.isnull(df.price)].index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores ausentes por coluna\n",
      "country        0\n",
      "description    0\n",
      "points         0\n",
      "price          0\n",
      "province       0\n",
      "variety        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Aplica função num_missing em todas as colunas\n",
    "print(\"Valores ausentes por coluna\")\n",
    "print(df.apply(num_missing, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU82LEyCEeGD"
   },
   "source": [
    "### 3.2 Criar objeto Dataset\n",
    "\n",
    "Para criar um pipeline de dados, a primeira etapa é criar um objeto Dataset a partir do DataFrame Pandas. \n",
    "\n",
    "Com um objeto Dataset é possível criar camadas tipo `feature_columns`, que servem como uma ponte entre os dados e a RNA. \n",
    "\n",
    "Lembre que se o arquivo de dados for muito grande, pode-se criar um Dataset carregando-o diretamente do arquivo no disco, lote por lote. Já vimos como fazer isso na Aula 5.\n",
    "\n",
    "Na célula abaixo é definida uma função para criar um Dataset a partir do DataFrame Pandas. Nessa função são realizadas as seguintes operações:\n",
    "\n",
    "1. Separação das saídas desejadas;\n",
    "2. Embaralhamento aleatóriamente dos dados, se for desejado;\n",
    "3. Geração de lotes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Is5CHLTJEeGE"
   },
   "outputs": [],
   "source": [
    "# Função para criar Dataset a partir de um DataFrame Pandas\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    # Cria cópia dpo Dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Separa saídas desejadas\n",
    "    labels = dataframe.pop('price')\n",
    "    \n",
    "    # Cria Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    \n",
    "    # Embaralha dados\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        \n",
    "    # Gera lotes de dados\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7NSuE1_EeGE"
   },
   "source": [
    "Vamos usar a função `df_to_datset` para criar o Dataset a partir do DataFrame Pandas `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mdxxTuniEeGE"
   },
   "outputs": [],
   "source": [
    "# Define tamanho do lote\n",
    "batch_size = 5\n",
    "\n",
    "# Cria Dataset\n",
    "ds = df_to_dataset(df, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oamG1K4CEeGF"
   },
   "source": [
    "Para verificar se o Dataset foi criado corretamente, vamos gerar um exemplo e observar suas características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7G_Z4EQEeGF",
    "outputId": "156e1f9a-f2ea-4012-fc1b-cc2f416dc1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista das características: ['country', 'description', 'points', 'province', 'variety']\n",
      "\n",
      "Um lote de country: [b'Italy' b'Italy' b'US' b'Austria' b'Italy']\n",
      "\n",
      "Um lote de description: [b'At the first it was quite muted and subdued, but over a ten minute period it developed beautifully in the glass. The dark fruit is still dominant, but it is beginning to show some interesting tertiary aromas of earth, mushrooms and leather. Well balanced with a medium finish. Needs a few more years, but can be enjoyed now with a good deal of pleasure.'\n",
      " b'Made with 65% Sangiovese, 20% Merlot and 15% Cabernet Sauvignon, this has subtle aromas of black-skinned fruit and thyme. The easygoing palate delivers black cherry and cinnamon alongside smooth tannins.'\n",
      " b\"Juicy plum, raspberry and pencil lead lead the way in this vineyard designate, a site that's 2,000 feet high. Tobacco and cedar meet a full-bodied hit of oak and puckering tannin, the wine still youthfully wrapped in its full-bodied boldness.\"\n",
      " b'Tender notes of red apple skin and bottled strawberries are pulled into focus by zesty citrus. The bone-dry palate reveals more bottled but wonderfully tart strawberry flavors and finishes in a very linear, refreshing and light-footed fashion.'\n",
      " b'This crisp sparkler is characterized by drying mineral tones and delicate aromas of stone fruit and citrus. The wine is foamy and slightly sweet in the mouth and does an excellent job of representing this category of Italian wine.']\n",
      "\n",
      "Um lote de points: [91 87 86 89 86]\n",
      "\n",
      "Um lote de variety: [b'Nebbiolo' b'Red Blend' b'Cabernet Sauvignon' b'Zweigelt' b'Prosecco']\n",
      "\n",
      "Um lote de saída (price): [70. 16. 75. 12. 18.]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in ds.take(1):\n",
    "    print('Lista das características:', list(feature_batch.keys()))\n",
    "    print('\\nUm lote de country:', format(feature_batch['country']))\n",
    "    print('\\nUm lote de description:', format(feature_batch['description']))\n",
    "    print('\\nUm lote de points:', format(feature_batch['points']))\n",
    "    print('\\nUm lote de variety:', format(feature_batch['variety']))\n",
    "    print('\\nUm lote de saída (price):', format(label_batch ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ--cSYyEeGG"
   },
   "source": [
    "- Observe que o Dataset retorna um dicionário de nomes das colunas do DataFrame, onde cada coluna representa uma característica e os seus valores se referem a cada vinho (uma linha de dados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uPo5DxoEeGG"
   },
   "source": [
    "### 3.3 Lote de exemplo\n",
    "\n",
    "Nos próximos itens vamos criar objetos `features_columns` que transformam os dados para uma forma que pode ser usada por uma RNA.\n",
    "\n",
    "Os objetos `feature_columns` são inseridos na RNA usando uma camada `Densefeatures` e os resultados das transformações são as entradas das camadas da rede, sem a necessidade de pré-processamento dos dados.  \n",
    "\n",
    "Para isso vamos usar um lote de dados de treinamento para exemplificar as transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Guy23necEeGH",
    "outputId": "e2b1e4db-a191-45ad-d323-a4db15bd9755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Italy', b'Germany', b'Australia', b'US', b'Chile'], dtype=object)>, 'description': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b\"The dynamic sisters that run this estate deliver a clean and territory-driven line of white wines. This easy Soave Classico opens with peach, pear, citrus and a touch of fragrant honeysuckle flower. It's pure and focused overall.\",\n",
      "       b'Just a touch of honey-lemon sweetness is enough to make this zesty, citrus-focused Riesling lip smackingly delicious. Fresh, green herb and lime notes reverberate through a strong finish. Drinks well now but should meld nicely over the next five years.',\n",
      "       b\"This deep brown wine smells like a damp, mossy cave. Then add complex rancio notes, plus maple syrup and molasses. It's full, round and harmonious, wonderfully rich yet without any sense of heaviness and long and bright on the finish.\",\n",
      "       b'Shows the briary, brambly character of Foothills Zin, with an array of wild berry, tobacco, mocha and Asian spice flavors wrapped into finely ground tannins. But the wine is too sweet and candied in the finish. Drink now.',\n",
      "       b\"Aromas of pumpkin, squash and corn chips are stale and not inviting. There's an acceptable mouthfeel to this weird, unbalanced Chardonnay along with flavors of spiced squash, mealy apple and saut\\xc3\\xa9ed root vegetables.\"],\n",
      "      dtype=object)>, 'points': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([87, 91, 98, 85, 80], dtype=int64)>, 'province': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'Veneto', b'Mosel', b'Victoria', b'California', b'Leyda Valley'],\n",
      "      dtype=object)>, 'variety': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
      "array([b'Garganega', b'Riesling', b'Muscadelle', b'Zinfandel',\n",
      "       b'Chardonnay'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "# Gera um lote de dados de entrada do conjunto de treinamento\n",
    "example_batch = next(iter(ds))[0]\n",
    "\n",
    "# Mostra lote gerado\n",
    "print(example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaU6sH3XEeGH"
   },
   "source": [
    "### 3.4 Visualização dos resultados das `feature_columns`\n",
    "\n",
    "`Feature_columns` são objetos que representam uma camada de uma RNA. Assim, para poder visualizar os resultados das transformações é necessário criar uma rede neural com uma camada do tipo `DenseFeatures` que aplica a transformação definida para processar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nxgK_Ej1EeGH"
   },
   "outputs": [],
   "source": [
    "# Importa módulo feature_column e a classe de camadas\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Cria RNA de uma camada para processar dados e visualizar transformação definida\n",
    "def rna_demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFRjeokLEeGH"
   },
   "source": [
    "- Usando essa `rna_demo` podemos visualizar claramente como cada coluna dos dados é transformada.\n",
    "\n",
    "\n",
    "- Veremos mais detalhes das camadas tipo `DenseFeatures` quando formos criar uma RNA com esse tipo de camada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvrTqEYHEeGI"
   },
   "source": [
    "### 3.5 Valores numéricos\n",
    "\n",
    "Uma coluna numérica (https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) é a forma mais simples de um objeto `feature_column`. Ela é usada para representar valores numéricos. Quando essa coluna é usada, a RNA recebe os valores da coluna inalterados.\n",
    "\n",
    "O método utilizado para fazer essa transformação é o `tf.feature_column.numeric_column()`. Nesse tipo de `feature_column` a RNA recebe os valores do Dataset sem nenhuma transformação.\n",
    "\n",
    "Os dados de entrada de uma RNA devem ser normalizados, assim, devemos normalizar os valores de points. Quando calculamos as estaíticas dos dados numéricos desse conjunto de dados, vimos que o valor mínimo de \"points\" é 80 e o valor máximo é 100.\n",
    "\n",
    "Vamos aplicar a seguinte normalização para a característica \"points\" de forma que os seus valores normalizados fiquem entre 0 e 1.\n",
    "\n",
    "$$point_{norm} = \\frac {point - point_{min}} {point_{max} - point_{min}}$$\n",
    "\n",
    "Para normalizar dados numéricos temos que definir uma função que realiza essa tranformação e ao criar a coluna numérica devemos chamar essa função.\n",
    "\n",
    "Para criar uma coluna numérica da coluna `points`, com valores normalizados, se faz o seguinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh7gK_VdEeGI",
    "outputId": "f09cae81-3f32-4370-c926-e6097adac4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35]\n",
      " [0.55]\n",
      " [0.9 ]\n",
      " [0.25]\n",
      " [0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Define função de normalização\n",
    "def norm_points(col):\n",
    "    min_points = 80\n",
    "    max_points = 100\n",
    "    return (col - min_points)/(max_points - min_points)\n",
    "\n",
    "# Criação da coluna \"points\" normalizada\n",
    "points_norm = tf.feature_column.numeric_column(\"points\", normalizer_fn=norm_points)\n",
    "\n",
    "# Apresenta resultados da transformação e a idade original\n",
    "rna_demo(points_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrzs2cYuEeGI"
   },
   "source": [
    "### 3.6 Segmentação de valores\n",
    "\n",
    "Frequentemente, não queremos fornecer um número diretamente à RNA, mas sim dividir seu valor em diferentes categorias com base em intervalos numéricos. Considere novamente a coluna de pontuação (\"points), podemos dividi-la em vários segmentos usando uma coluna segmentada. Por exemplo, usando 4 intervalos:\n",
    "\n",
    "    pointos < 85        --> [1, 0, 0, 0]\n",
    "    85 <= pointos < 90  --> [0, 1, 0, 0]\n",
    "    90 <= pointos < 95  --> [0, 0, 1, 0]\n",
    "    pointos >= 95       --> [0, 0, 0, 1]\n",
    "\n",
    "Nessa codificação cada faixa de pointos é reprentada por um vetor de 4 elementos, que representa a faixa de pontos de cada vinho. Os intervalos incluem o limite esquerdo e excluem o limite direito. \n",
    "\n",
    "O método utilizado para fazer essa transformação é o `tf.feature_column.bucketized_column()` (https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). \n",
    "\n",
    "O códico abaixo mostra como fazer essa transformação. Observe que antes de segmentar a pontuação temos que transformá-la em uma coluna numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nnw3Ems3JHVh",
    "outputId": "4dcfe3a8-74c0-45dc-c5d4-40591fe3f668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transforma idade em coluna numérica\n",
    "points = tf.feature_column.numeric_column(\"points\")\n",
    "\n",
    "# Segmenta idade\n",
    "points_seg = tf.feature_column.bucketized_column(points, boundaries=[85, 90, 95])\n",
    "\n",
    "# Apresenta resultados da segmentação\n",
    "rna_demo(points_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz3GoRtWJHVj"
   },
   "source": [
    "### 3.7 Váriaveis categóricas (codificação \"one-hot\")\n",
    "\n",
    "Neste conjunto de dados, as características \"country\", \"province\" e \"variety\" são representadas por strings. Não podemos alimentar strings diretamente em uma RNA. Em vez disso, devemos primeiro mapeá-las para valores numéricos. \n",
    "\n",
    "Para transformar dados categóricos na forma de strings para números inteiros tem-se dois métodos:\n",
    "\n",
    "- `tf.feature_column.categorical_column_with_vocabulary_list` (https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list); e\n",
    "- `tf.feature_colums.categorical_column_with_vocabulary_file` (https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file). \n",
    "\n",
    "No primeiro método o vocabulário (valores) é passado como uma lista e no segundo é carregado de um arquivo.\n",
    "\n",
    "Após mapear as categorias em números inteiros, elas devem ser codificados para vetores one-hot com dimensão igual ao número de categorias. \n",
    "\n",
    "Lembre que a codificação one-hot é muito usada em problemas de classificação multiclasse, onde as classes dos objetos em geral são especificadas ou por nomes ou números inteiros.\n",
    "  \n",
    "O método utilizado para realizar a codificação \"one-hot\" é `tf.feature_column.indicator_column` (https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column). O código a seguir mostra como realizar essa transformação para as características \"country\" e \"province\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ViYbaXRmJHVk",
    "outputId": "c8802a96-4ee0-44f3-ff70-f00052e39bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de categorias: 18\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transforma coluna \"country\" strings em categorias\n",
    "country = tf.feature_column.categorical_column_with_vocabulary_list('country', \n",
    "          vocabulary_list=df['country'].unique(), default_value=0)\n",
    "\n",
    "# Transforma números inteiros em vetores one-hot\n",
    "country_hot = tf.feature_column.indicator_column(country)\n",
    "\n",
    "# Número de categorias e \"country\"\n",
    "print('Número de categorias:', len(df.country.unique()))\n",
    "\n",
    "# Apresenta resultados da codificação\n",
    "rna_demo(country_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe que na transformação das strings em inteiros, o vocabulário foi passado pela própria coluna \"country\" do dataframe. Essa forma de passar o vocabulário facilita muito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de categorias: 95\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transforma coluna \"country\" strings em categorias\n",
    "province = tf.feature_column.categorical_column_with_vocabulary_list('province', \n",
    "          vocabulary_list=df['province'].unique(), default_value=0)\n",
    "\n",
    "# Transforma números inteiros em vetores one-hot\n",
    "province_hot = tf.feature_column.indicator_column(province)\n",
    "\n",
    "# Número de categorias e \"country\"\n",
    "print('Número de categorias:', len(df.province.unique()))\n",
    "\n",
    "# Apresenta resultados da codificação\n",
    "rna_demo(province_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATfAbaH-3aGI"
   },
   "source": [
    "### 3.8 Codificação \"hash\"\n",
    "\n",
    "Outra maneira de representar uma coluna categórica com um grande número de valores é usar o método `categorical_column_with_hash_bucket()`. \n",
    "\n",
    "Uma função \"hash\" é um algoritmo que mapeia dados de comprimento variável para dados de comprimento fixo. Para obter mais detalhes sobre essa forma de codificação ver o link: https://www.cs.cmu.edu/~adamchik/15-121/lectures/Hashing/hashing.html.\n",
    "\n",
    "Nesse método é calculado um vetor \"hash\" das categorias e depois esse vetor é segmentado com o número de intervalos desejados. \n",
    "\n",
    "Esse método não exige fornecer a lista de vocabulário e pode-se escolher o número de intervalos (\"hash_buckets\"). Com isso pode-se ter um número de elementos significativamente menor do que o número de categorias reais para economizar espaço.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "- Ao escolher um número de segmentos (\"hash_buckets\") menor do que o número de categorias reais, estamos forçando as diferentes categorias para um conjunto menor de códigos. \n",
    "\n",
    "- Isso significa que duas categorias não relacionadas podem ser mapeadas com o mesmo código e, conseqüentemente, passam a significar a mesma coisa para a rede neural $\\to$ nesse caso, pode haver colisões nas quais diferentes categorias são mapeadas para o mesmo intervalo. \n",
    "\n",
    "- Na prática, isso pode causar problemas e deve ser usado com cuidado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB9hl14f3aGJ"
   },
   "source": [
    "A transformação \"hash\" é realizada com o método `tf.feature_column.categorical_column_with_hash_bucket()` da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IEy8uLchJHVs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificação hashed da coluna province\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Transformação hashed\n",
    "province_hashed = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "      'province', hash_bucket_size=30)\n",
    "\n",
    "# Para facilitar a visualização da codificação hashed devemos transformá-la em um vetor one-hot\n",
    "print('Codificação hashed da coluna province')\n",
    "rna_demo(tf.feature_column.indicator_column(province_hashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqzZ9HRmJHVv"
   },
   "source": [
    "### 3.9 Cruzamento de características\n",
    "\n",
    "Combinar caracteríticas, também conhecido como cruzamentos de características, permite expandir os dados de entrada de forma a criar novas características que sejam mais significativas para o problema.\n",
    "\n",
    "Combinar caracteríticas também permite diminuir os dados de entrada ao usarmos o cruzamento de duas características no lugar de usar as duas, quando as duas características possuem informações redundantes.\n",
    "\n",
    "Pode-se criar um cruzamento de características a partir dos seguintes tipos de dados:\n",
    "\n",
    "- Qualquer dado categórico;\n",
    "- Qualquer dado numérico;\n",
    "- Qualquer dado segmentado.\n",
    "\n",
    "Existem muitas formas de cruzar características. A forma utilizada depende do problema e do tipo de dados sendo cruzados. Algumas formas de cruzar caracteríticas são as seguintes:\n",
    "\n",
    "1. Para dados numéricos pode-se multiplicar elemento por elemento;\n",
    "\n",
    "\n",
    "2. Para dados numéricos pode-se ajustar um polinômio de grau \"n\" aos dados;\n",
    "\n",
    "\n",
    "3. Para dados categóricos (ou segmentados) pode-se criar uma tabela 2D com as categorias de cada característica sendo as linhas e as colunas da tabela. Nesse caso, por exemplo, se tivermos uma caraterística com 5 classes e a outra com 10 classes, teremos no cruzamento das duas caraterísticas 50 categorias possíveis. \n",
    "\n",
    "\n",
    "#### Cruzamento de dados categóricos ou segmentados\n",
    "\n",
    "Ao cruzar duas características categóricas ou segmentadas, em princípio criamos uma nova característica representada por um vetor cuja dimensão é igual ao produto dos números de categorias/segmentos das duas características. \n",
    "\n",
    "Na maior parte dos casos uma grade completa é tratável apenas para entradas com vocabulários limitados, assim, em vez de construir uma tabela completa (potencialmente enorme) ao criarmos uma característica cruzada podemos escolher o número de novas categorias.\n",
    "\n",
    "A diminuição do número de categorias do cruzamento de duas características pode ser feita usando a codificação \"hash\". Porém, como vimos, usar a função \"hash\" limita o número de categorias, mas pode causar colisões de categorias, ou seja, vários cruzamentos de categorias acabarão no mesmo intervalo \"hash\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJiYfyZy3aGL"
   },
   "source": [
    "O cruzamento de duas características é feito com o método `tf.feature_column.crossed_column` (https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column). \n",
    "\n",
    "Esse método não cria uma tabela completa de todas as combinações possíveis. O resultado do cruzamento de características é processado por uma codificação Hash de forma a não criar a tabela completa de todas as combinações possíveis. Assim, pode-se escolher o número de elementos resultantes do cruzamento.\n",
    "\n",
    "Como exemplo, vamos criar uma nova careterística cruzando as colunas \"country\" e \"province\".\n",
    "\n",
    "Antes de realizar esse cruzamento vamos verificar o número de categorias dessas duas características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de países: 18\n",
      "Numero de provincias: 95\n"
     ]
    }
   ],
   "source": [
    "print('Número de países:', len(df.country.unique()))\n",
    "print('Numero de provincias:', len(df.province.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos:\n",
    "\n",
    "- Número de categorias de \"country\" = 18;\n",
    "- Número de categorias de \"province\" = 95;\n",
    "\n",
    "Portanto, a combinação dessas duas categorias sem perda nennhuma de informação gera uma nova característica com 18*95 = 1710 categorias. Porém vamos escolher somente 200 categorias no resultado do cruzamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57RH1upIJHVx",
    "outputId": "9c41b06c-8ce4-4008-c028-6f3b2b4e3d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado do cruzamento\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[0;32m    165\u001b[0m           tensor = column.get_dense_tensor(\n\u001b[1;32m--> 166\u001b[1;33m               transformation_cache, self._state_manager, training=training)\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_dense_tensor() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2352\u001b[0m       transformed = column.transform_feature(\n\u001b[1;32m-> 2353\u001b[1;33m           self, state_manager, training=training)\n\u001b[0m\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2352\u001b[0m       transformed = column.transform_feature(\n\u001b[1;32m-> 2353\u001b[1;33m           self, state_manager, training=training)\n\u001b[0m\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-bb1a6a63c768>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Processa a característica crossed_feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Resultado do cruzamento'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrna_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrossed_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-1b647819a834>\u001b[0m in \u001b[0;36mrna_demo\u001b[1;34m(feature_column)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrna_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfeature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m           tensor = column.get_dense_tensor(transformation_cache,\n\u001b[1;32m--> 169\u001b[1;33m                                            self._state_manager)\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[0mprocessed_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_dense_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcols_to_output_tensors\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget_dense_tensor\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   4156\u001b[0m     \u001b[1;31m# Feature has been already transformed. Return the intermediate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4157\u001b[0m     \u001b[1;31m# representation created by transform_feature.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtransformation_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m       \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Column {} is not supported.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   4093\u001b[0m     \"\"\"\n\u001b[0;32m   4094\u001b[0m     id_weight_pair = self.categorical_column.get_sparse_tensors(\n\u001b[1;32m-> 4095\u001b[1;33m         transformation_cache, state_manager)\n\u001b[0m\u001b[0;32m   4096\u001b[0m     return self._transform_id_weight_pair(id_weight_pair,\n\u001b[0;32m   4097\u001b[0m                                           self.variable_shape[-1])\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget_sparse_tensors\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   3950\u001b[0m     \u001b[1;34m\"\"\"See `CategoricalColumn` base class.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3951\u001b[0m     return CategoricalColumn.IdWeightPair(\n\u001b[1;32m-> 3952\u001b[1;33m         transformation_cache.get(self, state_manager), None)\n\u001b[0m\u001b[0;32m   3953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3954\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m       \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Column {} is not supported.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   3911\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[0mnum_buckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash_bucket_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3913\u001b[1;33m         hash_key=self.hash_key)\n\u001b[0m\u001b[0;32m   3914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3915\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross_hashed\u001b[1;34m(inputs, num_buckets, hash_key, name)\u001b[0m\n\u001b[0;32m    676\u001b[0m       \u001b[0mnum_buckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_buckets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m       \u001b[0mhash_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhash_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\u001b[0m in \u001b[0;36m_sparse_cross_internal\u001b[1;34m(inputs, hashed_output, num_buckets, hash_key, name)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m       \u001b[0minternal_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minternal_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross\u001b[1;34m(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name)\u001b[0m\n\u001b[0;32m   1059\u001b[0m           \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashed_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashed_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m           \u001b[0mnum_buckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhash_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m           internal_type=internal_type, name=name, ctx=_ctx)\n\u001b[0m\u001b[0;32m   1062\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross_eager_fallback\u001b[1;34m(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name, ctx)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   internal_type)\n\u001b[0;32m   1138\u001b[0m   _result = _execute.execute(b\"SparseCross\", 3, inputs=_inputs_flat,\n\u001b[1;32m-> 1139\u001b[1;33m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[0;32m   1140\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m     _execute.record_gradient(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "# Transforma coluna \"country\" strings em categorias\n",
    "country = tf.feature_column.categorical_column_with_vocabulary_list('country', \n",
    "          vocabulary_list=df['country'].unique(), default_value=0)\n",
    "\n",
    "# Transforma coluna \"country\" strings em categorias\n",
    "province = tf.feature_column.categorical_column_with_vocabulary_list('province', \n",
    "          vocabulary_list=df['province'].unique(), default_value=0)\n",
    "\n",
    "# Cria nova caracterítica pelo cruzamento das categorias de contry e province\n",
    "crossed_feature = tf.feature_column.crossed_column([country, province], hash_bucket_size=20)\n",
    "\n",
    "# Processa a característica crossed_feature\n",
    "print('Resultado do cruzamento')\n",
    "rna_demo(tf.feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBbihRrdJHVm"
   },
   "source": [
    "### 3.10 Codificação de texto (Embedding) \n",
    "\n",
    "Existem duas formas de codificar textos.\n",
    "\n",
    "#### Codificação de texto com \"one-hot\"\n",
    "\n",
    "A codificação de palavras (ou letras) de um texto pode ser realizada pelo método \"one-hot\". Porém, essa forma de codificação deve ser usada somente quando se tem um dicionário de palavras pequeno.\n",
    "\n",
    "\n",
    "#### Codificação de texto com \"embedding\"\n",
    "\n",
    "Suponha que em vez de ter apenas algumas categorias tenhamos milhares (ou mais). Por uma série de razões, na medida em que o número de categorias cresce, torna-se inviável treinar uma rede neural usando codificação \"one-hot\". \n",
    "\n",
    "Esse tipo de problema ocorre frequentemente em processamento de texto, onde pode-se ter um dicionário com milhares de palavras.\n",
    "\n",
    "Nesses casos se usa a codificação \"embedding\" para superar essa limitação. \n",
    "\n",
    "Em vez de representar as categorias como um vetor de dimensão grande com um único elemento diferente de zero, a codificação \"embedding\" representa cada categoria por um vetor denso com dimensão menor do que o número de categorias, onde cada elemento contém um número real.\n",
    "\n",
    "O tamanho do vetor na codificação \"embedding\" é um parâmetro que deve ser ajustado para o problema que se está resolvendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cufmldJ9JHVn"
   },
   "source": [
    "A codificação \"embedding\" é realizada com o método `tf.feature_column.embedding_column()` (https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column). \n",
    "\n",
    "Para exemplificar, vamos codificar a coluna \"variety\" para \"one-hot\" e \"embedding\", e analisar a diferença. A codificação \"one-hot\" é realizada somente para comparação e não precisa ser realizada para obter a codificação \"embedding\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkuqoP3TJHVn"
   },
   "outputs": [],
   "source": [
    "# Número de categorias da coluna \"variety\"\n",
    "print('Número de variedades:', len(df.variety.unique()))\n",
    "\n",
    "# Transforma strings em números inteiros\n",
    "variety = tf.feature_column.categorical_column_with_vocabulary_list('variety', \n",
    "           vocabulary_list=df['variety'].unique(), default_value=0)\n",
    "\n",
    "# Transformação one-hot \n",
    "variety_hot = tf.feature_column.indicator_column(variety)\n",
    "\n",
    "# Transformação \"embedding\"\n",
    "variety_embedding = tf.feature_column.embedding_column(variety, dimension=10)\n",
    "\n",
    "# Apresentação do resultado das codificações e da coluna conceito original\n",
    "print('\\nCodificação \"one-hot\"')\n",
    "rna_demo(variety_hot)\n",
    "print(' ')\n",
    "\n",
    "print('Codificação \"embedding\"')\n",
    "rna_demo(variety_embedding)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqqvdOgDJHVr"
   },
   "source": [
    "- Note que a entrada para transformação \"embedding\" são os dados categóricos obtidos na transformação com o método  `tf.feature_column.categorical_column_with_vocabulary_list()`.\n",
    "\n",
    "\n",
    "- Como temos 130 categorias na característica \"variety\", então a codificação one-hot dessa característica gera vetores de 130 elementos para cada categoria.\n",
    "\n",
    "\n",
    "- O argumento `dimension` é a dimensão desejada para os vetores de codificação \"embedding\", que nesse exemplo foi escolhido igual a 10.\n",
    "\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "- A codificação \"embedding\" é aprendida durante o treinamento da RNA juntamente com os parâmetros das camadas da RNA.\n",
    "\n",
    "- Nos vetores de codificação \"embedding\", os valores são números aleatórios usados para iniciar o processo de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49SQOCKUJHVs"
   },
   "source": [
    "#### Comparação \"one-hot\" e \"embedding\"\n",
    "\n",
    "Vejamos um exemplo de comparação de codificação \"one-hot\" e \"embeding\". Suponha que os exemplos de entrada consistam de uma única palavra de um conjunto de 81 palavras possíveis. Suponha ainda que temos 4 exemplos com as seguintes categorias:\n",
    "\n",
    "    \"dog\"\n",
    "    \"spoon\"\n",
    "    \"acissors\"\n",
    "    \"guitar\"\n",
    "\n",
    "Nesse caso, a Figura 1 ilustra a codificação desses 4 exemplos usando \"one-hot\" e \"embeding\".\n",
    "\n",
    "<br>\n",
    "<img src=\"Fig_Aula3_OneHot_Embeding.png\">\n",
    "<center>Figura 1- Codificação one-hot e embedding (https://medium.com/ml-book)</center>\n",
    "<br>\n",
    "\n",
    "Na codificação \"embedding\" ao usar números reais, pode-se ter um vetor com um número de elementos muito menor do que na codificação \"one-hot\". Os valores que aparecem nos vetores \"embedding\" são números aleatórios usados para inicializar o treinamento. Os valores mais adequados para o problema são determinados durante o treinamento.\n",
    "\n",
    "Quando um exemplo é processado, o método `tf.feature_column.categorical_column_with_vocabulary_list()` mapeia cada categoria (string) dos exemplos para um valor numérico entre 0 e número de categorias menos 1. Por exemplo, a categoria  “colher” é a de número 32. A partir dessa codificação inicial pode-se escolher codificar as categorias de duas formas diferentes:\n",
    "\n",
    "- \"One-hot\". Nesse caso a codificação converte cada valor categórico numérico em um vetor de 81 elementos (porque nosso conjunto de dados possui 81 palavras), colocando 1 no elemento de índice igual ao valor categórico (0, 32, 79, 80) e 0 em todas as outras posições.\n",
    "\n",
    "\n",
    "- \"Embedding\". Nesse exemplo a codificação \"embeding\" usa os valores categóricos numéricos (0, 32, 79, 80) como índices para criar vetores que contém 3 elementos.\n",
    "\n",
    "\n",
    "Como os valores nos vetores \"embeddings são atribuídos? As atribuições acontecem durante o treinamento, ou seja, o modelo aprende a melhor maneira de mapear seus valores categóricos numéricos de entrada para o valor do vetor de \"embedding\" para resolver o problema. \n",
    "\n",
    "A codificação \"embedding\" aumenta a capacidade de generalização da RNA, uma vez que um vetor \"embedding\" aprende novos relacionamentos entre as categorias a partir dos dados de treinamento.\n",
    "\n",
    "Para permitir um melhor entendimento, por exemplo, no caso de palavras em um problema de tradução de texto, pode-se imaginar que a codificação \"embedding\" representa, por exemplo, o tipo de palavra (substantivo, verbo, pronome, adjetivo etc), o seu significado e a sua frequência de uso. Seria como um \"dicionário digital\" das palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline para dados estruturados\n",
    "\n",
    "Vimos como funcionam alguns tipos de `feature_columns` para dados estruturados. Vamos agora preparar os dados para serem usados por uma RNA e mostrar um exemplo completo, desde a transformação dos dados, passando pelo treinamento da RNA, até a utilização da rede com dados brutos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXIIq4ih3aFy"
   },
   "source": [
    "### 4.1 Conjunto de dados\n",
    "\n",
    "Para exemplificar, vamos utilizar um pequeno conjunto de dados de doenças de coração disponibilizado pela Cleveland Clinic Foundation for Heart Disease (https://archive.ics.uci.edu/ml/datasets/heart+Disease).\n",
    "\n",
    "Esses dados estão em um arquivo tipo CSV. Cada linha representa um paciente e cada coluna descreve uma característica do paciente. O objetivo desses dados é prever se os pacientes apresentam ou não doença de coração, assim, consiste de um problema de classificação binária. \n",
    "\n",
    "Uma descrição dos dados desse conjunto é fornecida em https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names e um resumo segue abaixo. Note que existem dados numéricos e categóricos.\n",
    "\n",
    "\n",
    ">Column| Description| Feature Type | Data Type\n",
    ">------------|--------------------|----------------------|-----------------\n",
    ">Age | Age in years | Numerical | integer\n",
    ">Sex | (1 = male; 0 = female) | Categorical | integer\n",
    ">CP | Chest pain type (0, 1, 2, 3, 4) | Categorical | integer\n",
    ">Trestbpd | Resting blood pressure (in mm Hg on admission to the hospital) | Numerical | integer\n",
    ">Chol | Serum cholestoral in mg/dl | Numerical | integer\n",
    ">FBS | (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) | Categorical | integer\n",
    ">RestECG | Resting electrocardiographic results (0, 1, 2) | Categorical | integer\n",
    ">Thalach | Maximum heart rate achieved | Numerical | integer\n",
    ">Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical | integer\n",
    ">Oldpeak | ST depression induced by exercise relative to rest | Numerical | integer\n",
    ">Slope | The slope of the peak exercise ST segment | Numerical | float\n",
    ">CA | Number of major vessels (0-3) colored by flourosopy | Numerical | integer\n",
    ">Thal | 3 = normal; 6 = fixed defect; 7 = reversable defect | Categorical | string\n",
    ">Target | Diagnosis of heart disease (1 = true; 0 = false) | Classification | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ2_le6sEeF-"
   },
   "source": [
    "####  Carregar conjunto de dados\n",
    "\n",
    "Vamos usar o Pandas para carregar os dados de um arquivo CSV e carregá-lo em um objeto Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "-wkRDn8eEeF-",
    "outputId": "4057bd48-38a7-458c-b309-2f684dd1f854"
   },
   "outputs": [],
   "source": [
    "# Carrega dados em um DataFrame Pandas\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Apresenta DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYeYwtMBEeF_"
   },
   "source": [
    "- Observa-se que existem dados de 303 pacientes, ou seja, existem 303 exemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fikaH5jEeGA"
   },
   "source": [
    "Vamos verificar o tipo de dados e calcular as principais estatísticas dos dados de cada coluna do DataFrame para entender um pouco como são esses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "iUY5EamPEeGB",
    "outputId": "b37eeaff-f2a4-4eab-ab5a-ab15eff63923"
   },
   "outputs": [],
   "source": [
    "# principais estatísticas\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw2Rpo4HEeGB"
   },
   "source": [
    "#### Correção de erros\n",
    "\n",
    "Esse conjunto de dados tem dois dados com problema na coluna 'thal'. Vamos identificar esses exemplos com problemas e corrigir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3RZ7A2SEeGB",
    "outputId": "4b694ebb-6c94-4a07-e050-858622980de8"
   },
   "outputs": [],
   "source": [
    "# Valores existentes na coluna thal\n",
    "print(df['thal'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "daUVZPmaEeGB",
    "outputId": "5109308a-c164-46a0-f2b9-008c2ea7ec85"
   },
   "outputs": [],
   "source": [
    "# Identifica exemplos com erros\n",
    "df.loc[df.thal=='1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "Fw52jbcREeGC",
    "outputId": "07230f3b-0a48-483b-8c31-a346eb8de77b"
   },
   "outputs": [],
   "source": [
    "# Identifica exemplos com erros\n",
    "df.loc[df.thal=='2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ht3Gi57EeGC"
   },
   "outputs": [],
   "source": [
    "df['thal'] = df['thal'].replace(['1'],'normal')\n",
    "df['thal'] = df['thal'].replace(['2'],'reversible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NWVw_ONEeGC"
   },
   "source": [
    "### 4.2 Divisão dos dados em conjuntos de treinamento e teste\n",
    "\n",
    "Vamos dividir os dados em conjuntos de treinamento e teste. Observe que o conjunto de teste também vai ser usado como dados de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf_b01fuEeGD",
    "outputId": "9da85993-200c-4e4f-c19f-7398ab4b8b4b"
   },
   "outputs": [],
   "source": [
    "# Importa função para dividir dados da biblioteca ScikitLearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divisão dos dados\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Apresenta número de exemplos em cada conjunto\n",
    "print('Número de exemplos de treinamento:', len(df_train))\n",
    "print('Número de exemplos de teste:', len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU82LEyCEeGD"
   },
   "source": [
    "### 4.3 Criar objetos Dataset\n",
    "\n",
    "Para criar um pipeline de dados, a primeira etapa é criar um objeto Dataset a partir do DataFrame Pandas. \n",
    "\n",
    "Os dados em um objeto Dataset permite criar camadas tipo `feature_columns`, que servem como uma ponte entre os dados e a RNA. \n",
    "\n",
    "Lembre que se o arquivo de dados for muito grande, pode-se criar um Dataset carregando-o diretamente do arquivo no disco, lote por lote. Já vimos como fazer isso na Aula 4.\n",
    "\n",
    "Na célula abaixo é definida uma função para criar um Dataset a partir do DataFrame Pandas. Nessa função são realizadas as seguintes operações:\n",
    "\n",
    "1. Separação das saídas desejadas;\n",
    "2. Embaralhamento aleatóriamente dos dados, se for desejado;\n",
    "3. Geração de lotes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Is5CHLTJEeGE"
   },
   "outputs": [],
   "source": [
    "# Função para criar Dataset a partir de um DataFrame Pandas\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    # Cria cópia dpo Dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    # Separa saídas desejadas\n",
    "    labels = dataframe.pop('target')\n",
    "    \n",
    "    # Cria Dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    \n",
    "    # Embaralha dados\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        \n",
    "    # Gera lotes de dados\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7NSuE1_EeGE"
   },
   "source": [
    "Vamos usar a função `df_to_datset` para criar os Datasets de treinamento e teste a partir dos DataFrames Pandas (`df_train` e `df_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdxxTuniEeGE"
   },
   "outputs": [],
   "source": [
    "# Define tamanho do lote\n",
    "batch_size = 5\n",
    "\n",
    "# Cria Dataset de treinamento\n",
    "train_ds = df_to_dataset(df_train, batch_size=batch_size)\n",
    "\n",
    "# Cria Dataset de teste\n",
    "test_ds = df_to_dataset(df_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oamG1K4CEeGF"
   },
   "source": [
    "- Observa-se que não foi realizado o embaralhamento dos dados na criação dos Datasets porque na divisão dos dados nos conjutnos de treinamento e teste, usando a função `train_test_split()` da biblioteca ScikitLearn essa operação já é relizada.\n",
    "\n",
    "Para verificar se os Datasets de treinamento e teste foram criados corretamente, vamos gerar um exemplo e observar suas características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7G_Z4EQEeGF",
    "outputId": "156e1f9a-f2ea-4012-fc1b-cc2f416dc1de"
   },
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('Lista das características:', list(feature_batch.keys()))\n",
    "    print('Um lote de idade:', format(feature_batch['age']))\n",
    "    print('Um lote de sexo:', format(feature_batch['sex']))\n",
    "    print('Um lote de cp:', format(feature_batch['cp']))\n",
    "    print('Um lote de trestbps:', format(feature_batch['trestbps']))\n",
    "    print('Um lote de chol:', format(feature_batch['chol']))\n",
    "    print('Um lote de fbs:', format(feature_batch['fbs']))\n",
    "    print('Um lote de restecg:', format(feature_batch['restecg']))  \n",
    "    print('Um lote de thalach:', format(feature_batch['thalach']))   \n",
    "    print('Um lote de oldpeak:', format(feature_batch['oldpeak']))\n",
    "    print('Um lote de ca:', format(feature_batch['ca']))\n",
    "    print('Um lote de thal:', format(feature_batch['thal']))    \n",
    "    print('Um lote de saída:', format(label_batch ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ--cSYyEeGG"
   },
   "source": [
    "- Observe que o Dataset retorna um dicionário de nomes das colunas do DataFrame, onde cada coluna representa uma característica e os seus valores se referem a cada paciente (uma linha de dados)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uPo5DxoEeGG"
   },
   "source": [
    "### 4.4 Criar lote de exemplo\n",
    "\n",
    "Nos próximos itens vamos criar objetos `features_column` que transformam os dados para uma forma que pode ser usada por uma RNA.\n",
    "\n",
    "Os objetos `feature_column` são inseridos na RNA usando uma camada `Densefeature` e os resultados das transformações são as entradas das camadas da rede, sem a necessidade de pré-processamento dos dados.  \n",
    "\n",
    "Para isso vamos usar um lote de dados de treinamento para exemplificar as transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Guy23necEeGH",
    "outputId": "e2b1e4db-a191-45ad-d323-a4db15bd9755"
   },
   "outputs": [],
   "source": [
    "# Gera um lote de dados de entrada do conjunto de treinamento\n",
    "example_batch = next(iter(train_ds))[0]\n",
    "\n",
    "for exemple_batch in train_ds.take(1):\n",
    "    for key, value in example_batch.items():\n",
    "        print(\"  {}: {}\".format(key, value))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQI5YsWTEeGP"
   },
   "source": [
    "### 4.5 Transformação e preparação dos dados de entrada\n",
    "\n",
    "O trabalho \"Seleção de variáveis e classificação de padrões por redes neurais como auxílio ao diagnóstico de cardipatia isquêmica\" (https://www.scielo.br/pdf/pope/v28n2/07.pdf) apresenta as catacrerísticas mais importantes desse conjunto de dados para diagnosticar doença cardiaca. Usando os resultados desse trabalho vamos ignorar as seguintes características:\n",
    "\n",
    "- \"fbs\", \"sex\", \"chol\" e \"trestbps\"\n",
    "\n",
    "As transformações que iremos aplicar em cada uma das características são as seguintes:\n",
    "\n",
    "- \"age\": numérico $\\to$ segmentação [40, 50, 60, 70]\n",
    "- \"cp\": categoria (numérica) $\\to$ one-hot\n",
    "- \"restECG\": categoria (numérica) $\\to$ one-hot\n",
    "- \"thalach\": numérico $\\to$ normalização entre 0 e 1\n",
    "- \"exang\": categoria (0 ou 1)\n",
    "- \"oldpeak\": numerico $\\to$  segmentação [0.1, 0.81, 1.61]\n",
    "- \"slope\": categoria (numérica) $\\to$ one-hot\n",
    "- \"ca\": categoria (numérica) $\\to$ one-hot\n",
    "- \"thal\": categoria (string) $\\to$ one-hot\n",
    "\n",
    "Após definir cada uma das transformações desejadas nos dados de entrada temos que preparar esses dados para poderem ser entradas de uma RNA. Para isso temos inicialmente que definir uma lista com as colunas de características que queremos usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir cada uma das tranformações que queremos realizar nos dados e incluir em uma lista de características que queremos usar como dado de entrada da RNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incializa lista de características desejadas\n",
    "feature_columns_list = []\n",
    "\n",
    "# Inicializa número de entradas de cada exemplo\n",
    "num_input = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentação da coluna numerica \"age\"\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "seg_age = tf.feature_column.bucketized_column(age, boundaries=[40, 50, 60, 70])\n",
    "\n",
    "# Visulização de um lote\n",
    "rna_demo(seg_age)\n",
    "\n",
    "# Inclui seg_age na lista de características desejadas\n",
    "feature_columns_list.append(seg_age)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"cp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação one-hot dos dados categóricos da coluna \"cp\"\n",
    "cp = feature_column.categorical_column_with_vocabulary_list('cp', [0, 1, 2, 3, 4])\n",
    "cp_hot = tf.feature_column.indicator_column(cp)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(cp_hot)\n",
    "\n",
    "# Inclui cp_hot na lista de características desejadas\n",
    "feature_columns_list.append(cp_hot)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"restecg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação one-hot dos dados categóricos da coluna \"restecg\"\n",
    "restecg = feature_column.categorical_column_with_vocabulary_list('restecg', [0, 1, 2])\n",
    "restecg_hot = tf.feature_column.indicator_column(restecg)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(restecg_hot)\n",
    "\n",
    "# Inclui seg_age na lista de características desejadas\n",
    "feature_columns_list.append(restecg_hot)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"thalach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define função de normalização\n",
    "def norm_max(col):\n",
    "    return col/max_thalach\n",
    "\n",
    "# Define valor máximo de \"thalach\"\n",
    "max_thalach = 202\n",
    "\n",
    "# Normalização da coluna \"thalach\"\n",
    "thalach_norm = tf.feature_column.numeric_column(\"thalach\", normalizer_fn=norm_max)\n",
    "\n",
    "# Visualização de um lote \n",
    "rna_demo(thalach_norm)\n",
    "\n",
    "# Inclui thalach_norm na lista de características desejadas\n",
    "feature_columns_list.append(thalach_norm)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para normalizar dados numéricos temos que definir uma função que realiza essa tranformação e ao criar a coluna numérica devemos chamar essa função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"exang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação dos dados categóricos da coluna \"exang\"\n",
    "exang_cat = feature_column.categorical_column_with_vocabulary_list('exang', [0, 1])\n",
    "exang_cat = tf.feature_column.indicator_column(exang_cat)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(exang_cat)\n",
    "\n",
    "# Inclui exang_cat na lista de características desejadas\n",
    "feature_columns_list.append(exang_cat)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"oldpeak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentação da coluna \"oldpeak\"\n",
    "oldpeak = tf.feature_column.numeric_column(\"oldpeak\")\n",
    "seg_oldpeak = tf.feature_column.bucketized_column(oldpeak, boundaries=[0.1, 0.81, 1.61])\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(seg_oldpeak)\n",
    "\n",
    "# Inclui seg_oldpeak na lista de características desejadas\n",
    "feature_columns_list.append(seg_oldpeak)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"slope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação one-hot dos dados categóricos da coluna \"slope\"\n",
    "slope = feature_column.categorical_column_with_vocabulary_list('slope', [1, 2, 3])\n",
    "slope_hot = tf.feature_column.indicator_column(slope)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(slope_hot)\n",
    "\n",
    "# Inclui slope_hot na lista de características desejadas\n",
    "feature_columns_list.append(slope_hot)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"ca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação one-hot dos dados categóricos da coluna \"ca\"\n",
    "ca = feature_column.categorical_column_with_vocabulary_list('ca', [0, 1, 2, 3])\n",
    "ca_hot = tf.feature_column.indicator_column(ca)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(ca_hot)\n",
    "\n",
    "# Inclui cat_hot na lista de características desejadas\n",
    "feature_columns_list.append(ca_hot)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Característica \"thal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificação one-hoe das categorias da coluna \"thal\"\n",
    "thal = feature_column.categorical_column_with_vocabulary_list('thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_hot = tf.feature_column.indicator_column(thal)\n",
    "\n",
    "# Visualização de um lote\n",
    "rna_demo(thal_hot)\n",
    "\n",
    "# Inclui thal_hot na lista de características desejadas\n",
    "feature_columns_list.append(thal_hot)\n",
    "\n",
    "# Atualiza número de entradas\n",
    "num_input += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numero de entradas de cada exemplo:', num_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0rmyueUEeGQ"
   },
   "source": [
    "### 4.6 Criar camada de características (`DenseFeatures`)\n",
    "\n",
    "Após definir as colunas de características que queremos usar na RNA e as transformações desejadas, temos que criar uma camada do tipo `DenseFeatures` (https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures) para receber e transformar as caracteríticas desejadas.\n",
    "\n",
    "As características desejadas devem estar em uma lista, que no caso é a `feature_columns_list`.\n",
    "\n",
    "Uma camada do tipo `DenseFeatures` produz um tensor (vetor) baseado nas características e nas transformações definidas na lista de colunas de característica (`feature_columns_list`).\n",
    "\n",
    "A camada `DenseFeatures` deve ser a primeira camada da RNA após a camada `Input`, se esta existir.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Definir Dataset com lotes maiores\n",
    "\n",
    "Anteriormente, usamos um tamanho de lote pequeno para demonstrar como as `feature_column` funcionam. \n",
    "\n",
    "Vamos definir  agora um Dataset com um tamanho de lote maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tamanho do lote\n",
    "batch_size = 32\n",
    "\n",
    "# Cria Dataset de treinamento\n",
    "train_ds = df_to_dataset(df_train, batch_size=batch_size)\n",
    "\n",
    "# Cria Dataset de teste\n",
    "test_ds = df_to_dataset(df_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Configuração, compilação e treinamento da RNA\n",
    "\n",
    "Para resolver esse problema de calcular a probabilidade de um paciente ter doença do coração, vamos utilizar uma RNA simples com três camadas densas. \n",
    "\n",
    "Vamos compilar essa RNA vamos com o método de otimização Adams e depois vamos treiná-la usando 50 épocas usando os Datasets de treinamento e teste (`train_ds` e `test_ds`).\n",
    "\n",
    "Com é um problema de classificação binária, a função de custo mais indicada é a `binary_crossentropy` (função logística). Como métrica vamos utlizar a exatidão (`accuracy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "rna = Sequential()\n",
    "rna.add(feature_layer)\n",
    "rna.add(layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "rna.add(layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "rna.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "rna.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rna.fit(train_ds, validation_data=test_ds, epochs=70, verbose=0)\n",
    "\n",
    "rna.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações:**\n",
    "\n",
    "- Essa RNA possui 8.129 parâmetros treináveis;\n",
    "\n",
    "- No sumário da RNA não aprece o número de ativações de cada camada;\n",
    "\n",
    "- Não é possível usar o método `summary` antes de treinar a RNA porque as dimensões das entradas e ativações das camadas sómente é definida durante o treinamento, quando a RNA recebe os dados de entrada calculados pelas `feature_column`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostra gráficos do processo de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera resultados de treinamento do dicinário history\n",
    "acc      = results.history['accuracy']\n",
    "val_acc  = results.history['val_accuracy']\n",
    "loss     = results.history['loss']\n",
    "val_loss = results.history['val_loss']\n",
    "\n",
    "# Cria vetor de épocas\n",
    "epocas   = range(len(acc)) \n",
    "\n",
    "# Gráfico dos valores da função de custo\n",
    "plt.plot(epocas, loss, 'r', label='Custo - treinamento')\n",
    "plt.plot(epocas, val_loss, 'b', label='Custo - validação')\n",
    "plt.title('Valor da função de custo – treinamento e validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Custo')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Gráfico dos valores da métrica\n",
    "plt.plot(epocas, acc, 'r', label='exatidao- treinamento')\n",
    "plt.plot(epocas, val_acc, 'b', label='exatidao - validação')\n",
    "plt.title('Valor da métrica – treinamento e validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Exatidao')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Avaliação e teste da RNA\n",
    "\n",
    "Para verificar o desempenho da RNA vamos calcular o valor da função de custo e da métrica para os dois conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia desempenho da RNA para os dados de treinamento, validação e teste\n",
    "eval_train = rna.evaluate(train_ds, verbose=0)\n",
    "eval_test = rna.evaluate(test_ds, verbose=0)\n",
    "\n",
    "# Apresenta resultados\n",
    "print('Dados de treinamento: Função de custo =', eval_train[0], '- Exatidão =', eval_train[1])\n",
    "print('Dados de teste: Função de custo =', eval_test[0], '- Exatidão =', eval_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa-se que a RNA acerta se o paciente tem doença do coração em cerca de 84% dos casos nos exemplos de teste.\n",
    "\n",
    "#### Teste da RNA\n",
    "\n",
    "Para finalizar a avaliação, vamos verificar como a RNA prevê a probabilidade dos pacientes do conjunto de teste terem ou não doença de coração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gera um exemplos de dados do conjunto de teste\n",
    "for x, y in test_ds.take(1):\n",
    "    # Mostra lote gerado\n",
    "    #print('Entrada:\\n', x)\n",
    "\n",
    "    # calcula previsão da RNA\n",
    "    y_prev = np.round(rna.predict(x))\n",
    "    \n",
    "    # Mostra saída real e prevista\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.plot(y, 'ro', label='Classe real')\n",
    "    plt.plot(y_prev, 'bo', label='Classe prevista')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A6_Data_Pipeline_Parte3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
