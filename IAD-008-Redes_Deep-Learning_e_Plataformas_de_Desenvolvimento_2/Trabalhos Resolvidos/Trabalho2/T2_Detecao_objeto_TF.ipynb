{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6ZDpd9XzFeN"
   },
   "source": [
    "# Trabalho #2 - Detecção de objetos com API do TensorFlow\n",
    "\n",
    "Nesse trabalho você vai retreinar uma RNA pré-treinada, disponível na API de detecção de objetos do TensorFlow, para detectar novos tipos de objetos em imagens.\n",
    "\n",
    "### Referências\n",
    "\n",
    "O código usado nesse trabalho é baseado no Notebook do TensorFlow disponível em https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coloque o seu nome aqui:\n",
    "\n",
    "Nome:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC7S6iyqZNjt"
   },
   "source": [
    "## 1. Objetivos\n",
    "\n",
    "O objetivo desse trabalho é realizar um treinamento de detecção de objetos customizado usando uma RNA pré-treinada.\n",
    "\n",
    "O conjunto de dados consiste de imagens que mostram dois tipos de objetos: 1) cangurus e 2) pessoas. Assim, tem-se um problema de detecção de objetos com duas classes.\n",
    "\n",
    "A tarefa principal desse trabalho é preparar os dados para treinamento, ou seja, realizar as anotações das imagens. Para realizar essas anotações você vai usar a função `colab_utils.annotate` que usamos na aula. \n",
    "\n",
    "Para desenvolver esse trabalho, você vai ter que realizar algumas modificações no progrma usado para essa finalidade visto em aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZHqHe0pZNju"
   },
   "source": [
    "## 2. Instalação de bilbiotecas e da API de detecção de objetos \n",
    "\n",
    "### 2.1 Clonar o repostório de modelos do TensorFlow Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMMGdrg7ZNju"
   },
   "outputs": [],
   "source": [
    "# Importa bibliotecas do sistema operacional\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Clonagem do repositório de modelos do TensorFlow se isso ainda não foi realizado\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "    while \"models\" in pathlib.Path.cwd().parts:\n",
    "        os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "    !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSCiNoAaZNju"
   },
   "source": [
    "### 2.2 Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4op-FpCZNjv"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u15jitgLVsjt"
   },
   "source": [
    "### 2.3 Instalar a API de detecção de objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u6j9xHxZNju"
   },
   "outputs": [],
   "source": [
    "# Instalar API de detecção de objetos se isso ainda não foi realizado\n",
    "%%bash\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lKWtXJ0H-9e"
   },
   "source": [
    "### 2.4 Importar funções da API de detecção de objetos.\n",
    "\n",
    "Após a instalação da API, tem-se disponível bilbiotecas de detecção de objetos com classes e funções que devem ser importadas para o programa para poderem ser usadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epl9OPzyH-9e"
   },
   "outputs": [],
   "source": [
    "# Importa bilbiotecas da API de detecção de objetos\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util \n",
    "from object_detection.utils import colab_utils \n",
    "from object_detection.builders import model_builder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCSGEfm8H-9e"
   },
   "source": [
    "- `viz_utils` contém métodos para desenhar caixas delimitadoras.\n",
    "\n",
    "- `label_map_util` é uma função que associa números para os nomes das classes de objetos. Observa-se que os modelos geram números para as classes e com essa função é possível associar esses números com os nomes das classes.\n",
    "\n",
    "- `config_util` função para criar modelos a partir do arquivo de configuração\n",
    "\n",
    "- `colab_utils` função para anotar imagens\n",
    "\n",
    "- `model_builder` função para construir modelo e carregar parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC31xZH8ZNjv"
   },
   "source": [
    "### 2.5 Funções auxiliares carregar imagens e desenhar caixas delimitadoras\n",
    "\n",
    "Na célula abaixo são definidas duas funções, uma para carregar imagens de uma pasta e transformar em tensor Numpy e outra para desenhar as caixas delimitadoras identificadas sobrepostas na imagem. Para visualizar a imagem com as caixas é usado o método `viz_utils.visualize_boxes_and_labels_on_image_array()` da API de detecção de objetos. \n",
    "\n",
    "1. `load_img_into_numpy_array()` carrega imagem de uma pasta e a transfroma em tensor Numpy\n",
    "\n",
    "2. `plot_detection()` mostra uma imagem com as caixas delimitadoras sobrepostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fiDZuxuZNjv"
   },
   "outputs": [],
   "source": [
    "# Define função para carregar imagem e transformar em tensor Numpy\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "        path: a file path.\n",
    "\n",
    "    Returns:\n",
    "        uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "# Define função para mostrar imagem com caixas delimitadoras sobrepostas\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "    \"\"\"Wrapper function to visualize detections.\n",
    "\n",
    "    Args:\n",
    "        image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "        boxes: a numpy array of shape [N, 4]\n",
    "        classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
    "                 and match the keys in the label map.\n",
    "        scores: a numpy array of shape [N] or None.  If scores=None, then\n",
    "                this function assumes that the boxes to be plotted are groundtruth\n",
    "        boxes and plot all boxes as black with no classes or scores.\n",
    "        category_index: a dict containing category dictionaries (each holding\n",
    "                        category index `id` and category name `name`) keyed by category indices.\n",
    "        figsize: size for the figure.\n",
    "        image_name: a name for the image file.\n",
    "    \"\"\"\n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_annotations,\n",
    "          boxes,\n",
    "          classes,\n",
    "          scores,\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          min_score_thresh=0.8)\n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZYu2ui9ZNjw"
   },
   "source": [
    "## 3. Conjunto de dados\n",
    "\n",
    "Vamos usar o conjunto de dados originalmente utilizado para detectar cangurus. Porém como muitas imagens desse conjunto mostram também pessoas, vamos utilizá-lo para detectar cangurus e pessoas.\n",
    "\n",
    "As imagens desse conjunto de dados etá no arquivo compactado `cangurus_pessoas.zip`. Para carregar esse conjutno de dados primeiramente você que que importá-lo para o ambiente do Colab. \n",
    "\n",
    "Após importar o arquivo deve descompactá-lo executando a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIviWtMuXhx4"
   },
   "outputs": [],
   "source": [
    "!unzip cangurus_pessoas.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A descompactação desse arquivo gera uma pasta de nome \"cangurus_pessoas\" com duas sub pastas. Uma de nome \"train\", com as imagens usadas para treinamento, e outra de nome \"test\" com as imagens usadas para teste.\n",
    "\n",
    "Verifique no seu ambiente do Colab onde está a pasta \"cangurus_pessoas\". Para isso execute a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzN8VJuKaBTe"
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Carregar imagens de treinamento\n",
    "\n",
    "Para carregar as imagens de treinamento devemos gerar uma lista com os arquivos presentes na pasta \"train\". \n",
    "\n",
    "\n",
    "### Exercício #1: Gerar lista de arquivos das imagens\n",
    "\n",
    "Para carregar as imagens de treinamemto crie um código na célula abaixo para gerar um alista dos arquivos das imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfP4dDvpZddO"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: carregar imagens de treinamento\n",
    "\n",
    "# Importa bilbliotecas glob\n",
    "from glob import glob\n",
    "\n",
    "# Define diretório onde se encontram as imagens\n",
    "train_dir =\n",
    "\n",
    "# Escolhe tipos de arquivos desejados\n",
    "glob_imgs = \n",
    "\n",
    "# Cria lista dos nomes dos arquivos\n",
    "img_paths =\n",
    "\n",
    "# Mostra 5 primeiros arquivos da lista\n",
    "print(img_paths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #2: Carregar imagens de treinamento\n",
    "\n",
    "As imagens de treinamento devem carregas, transformadas em tensores Numpy e colocadas na lista `train_images_np`. Para carregar e transfromar as imagens em tensores Numpy use a função `load_image_into_numpy_array` defina na Seção 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2HNBxLHZNjw"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Carregar e visualizar imagens de treinamento\n",
    "\n",
    "# Iniciliza lista de imagens\n",
    "train_images_np = []\n",
    "\n",
    "# Carrega imagens, tranforma em tensores Numoy e inclui n alista\n",
    "# Insira seu código aqui\n",
    "#\n",
    "\n",
    "# Visualização das imagens\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = False\n",
    "plt.rcParams['ytick.labelsize'] = False\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['figure.figsize'] = [16, 60]\n",
    "\n",
    "for idx, train_image_np in enumerate(train_images_np):\n",
    "    plt.subplot(7, 3, idx+1)\n",
    "    plt.imshow(train_image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observa-se que existem 19 imagens no conjunto de treinamento. Esse número é muito pequeno para realizar um treinamento efetivo, porém a anotação dessas 19 imagens é bastante trabalhosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TRXKL2xZNjw"
   },
   "source": [
    "### 3.2  Anotação das imagens com as caixas delimitadoras\n",
    "\n",
    "Você tem que anotar as imagens identificando os cangurus e as pessoas. Com visto, isso representa desenhar uma caixa ao redor de cada canguru e cada pessoa presentes nas imagens. Ao fazer isso, são criadas as caixas delimitadoras desejadas para cada imagem, que representam as saídas desejadas.\n",
    "\n",
    "A API de detecção de objetos disponibiliza a função `colab_utilis.annotate()` para auxiliar a execução dessa tarefa. Infelizmente essa função é bastante limitada e não permite definir as classes de cada caixa, portanto, isso terá que ser realizado após serem definidas todas as caixas de todas as imagens.\n",
    "\n",
    "Oberva que existem muitos softwares para facilitar a anotação de imagens, um bstante completo e de uso livre para poucas iamgens é o Roboflow (https://roboflow.com/).\n",
    "\n",
    "**Importante:** a qualidade do seu modelo de detecção de objetos depende muito da realização de um trabalho de anotação das imagens. Dessa forma, tente ser o mais preciso possível nessa tarefa.\n",
    "\n",
    "\n",
    "### Exercício #3; Anotação das imagens de treinamento\n",
    "\n",
    "Execute a célula abaixo para realizar as anotações das 19 imagens de treinamento.\n",
    "\n",
    "#### Dicas:\n",
    "\n",
    "1. Utlize classe 0 para os cangurus e classe 1 para as pessoas.\n",
    "2. Para facilitar a definição das classes de cada caixa de cada imagem, para cada imagem defina primeiro todas as caixas dos cangurus para depois definir as caixas das pessoas. Isso vai facilitar associar para cada caixa de cada imagem a sua classe correspondente.\n",
    "3. Após criar a lista de classes é interessante copiá-la para poder usá-l posteriormenet e assim, evitar de ter que repetir essa tarefa de anotação das imagens toda vez que tiver que refazer o trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxeNgPi6ZNjx"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Anotação das imagens\n",
    "\n",
    "# Inicializar a lista de caixas\n",
    "gt_boxes = []\n",
    "\n",
    "# Anotação manual das imagens para criar caixas\n",
    "# Insira seu código aqui\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lista de caixas identificadas nas imagens é criada na variável `gt_boxes` e é apresentada executando a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PkZUJdDZNjx"
   },
   "outputs": [],
   "source": [
    "# Apresenta lista de caixas geradas na anotação das imagens\n",
    "gt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLwV-e-QZNjx"
   },
   "source": [
    "### Exercíco #4: Definição das classes das caixas\n",
    "\n",
    "Você tem que associar cada caixa presente nas 19 imagens com a sua classe e criar uma lista com as classes de todas as imagens.\n",
    "\n",
    "As classes são representadas por números inteiros. No caso de se ter duas classes, pode-se definir por exemplo:\n",
    "\n",
    "- classe = 0, para canguru;\n",
    "- classe = 1, para pessoa.\n",
    "\n",
    "Para cada imagem deve ser associada um vetor com as classes de cada caixa. Por exemplo, se a imagem 1 tem três caixas, sendo duas de cangurus e uma de pessoa, para essa imagem o vetor de classes deve ser o seguinte: `[0, 0, 1]`.\n",
    "\n",
    "A lista de classes é obtida simplesmente unindo os vetores de classes de cada imagem em uma variável tipo lista usando o método `append`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJxJje379nxQ"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Definição das classes das caixas\n",
    "\n",
    "# Incicializar lista de classes das imagens\n",
    "classes=[]\n",
    "\n",
    "# Definir vetor de classe de cada imagem e incluir na lista de classes\n",
    "# Insira seu código aqui\n",
    "#\n",
    "\n",
    "# Mostra lista de classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1V9eTRgZNjy"
   },
   "source": [
    "### 3.3 Preparar dados para o treinamento\n",
    "\n",
    "Os dados de treinamento devem ser preparados. Essa preparação envolve as seguintes etapas:\n",
    "\n",
    "1. Criar dicionário que associa número das classes com os seus nomes.\n",
    "2. Converter a imagens para tensor do TensorFlow e em números reais, e incluir eixo dos exemplos. Observa-se que a normalização das imagens é realizada dentro da rede.\n",
    "3. Converter lista de caixas para tensor do TensorFlow.\n",
    "4. Converter lista de classes para tensor do TensorFlow.\n",
    "5. Converter classes para vetores one-hot.\n",
    "\n",
    "\n",
    "### Exercício #5: Preparar dados\n",
    "\n",
    "Na célula abaixo execute as etapas de preparação dos dados descritas acima.\n",
    "\n",
    "Dicas:\n",
    "- Para converter uma imagem ou um vetor em um tensor TF use a função `tf.convert_to_tensor()`.\n",
    "- Ao converter uma imagem ou um vetor em tensor TF deve definir o tipo de dado, para as imagens e caixas usar `tf.float32` e para as classes `tf.int32`. \n",
    "- Para incluir o eixo dos exemplos nas imagens use a função `tf.expand_dims()`.\n",
    "\n",
    "Observa-se que não precisa usar a variável `label_id_offset`, como usado na aula, se identificou as classes com 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWLaLXYkZNjy"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Preparação dos dados de treinamento\n",
    "\n",
    "# Define número de classes\n",
    "num_classes = 2\n",
    "\n",
    "# Define dicionário com números e nomes das classes\n",
    "# Insira seu código aqui\n",
    "#\n",
    "\n",
    "\n",
    "# Os índices das classes devem ser convertidos para vetores one-hot e todas as variáveis devem ser\n",
    "# transformadas em tensores.\n",
    "\n",
    "# Incializa lista de imagens com números reais\n",
    "train_image_tensors = []\n",
    "\n",
    "# Inicializa lista de classes codificadas com vetores one-hot\n",
    "gt_classes_one_hot_tensors = []\n",
    "\n",
    "# Inicializa lista de caixas \n",
    "gt_box_tensors = []\n",
    "\n",
    "# Iterage em todas as imagens de treinamento incluindo as imagens, as caixas e as classes transfromadas nas listas\n",
    "for (train_image_np, gt_box_np, classe) in zip(train_images_np, gt_boxes, classes):\n",
    "    \n",
    "    # Incluiu eixo dos exemplos e transforma imagens em tensores com números reais (use axis=0)\n",
    "    train_image_tensors.append\n",
    "    \n",
    "    # Transforma caixas a serem previstas em tensores TF\n",
    "    gt_box_tensors.append\n",
    "    \n",
    "    # Transforma caixas reais em tensores TF\n",
    "    zero_indexed_groundtruth_classes = \n",
    "\n",
    "    # Codificação one-hot das classes\n",
    "    gt_classes_one_hot_tensors.append\n",
    "\n",
    "print('Preparação de dados pronta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização das classe codificadas em vetores one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFcceKtEnstc"
   },
   "outputs": [],
   "source": [
    "print(gt_classes_one_hot_tensors)\n",
    "x = np.array(classes[0])\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI3lrjaEZNjy"
   },
   "source": [
    "### 3.4 Visualização das imagens de treinamento com as caixas sobrepostas\n",
    "\n",
    "Para verificar se a preparação de dados foi realizada de forma correta, é importante visualizar as imagens com as caixas sobrepostas indentificadas pelas suas classes e pela probabilidade de estarem corretas.\n",
    "\n",
    "Para realizar essa visualização é necessário primeiramente definir uma lista de probabilidades (\"scores\") \"fake\", porque a função de visualização disponibilizada pelo TensorFlow exige esse dado de entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercíco #6: Definição dos \"scores\"\n",
    "\n",
    "Para poder visualizar as imagens de treinamento com as caixas sobrepostas deve-se associar cada caixa presente nas 19 imagens de treinamento com a probabilidade da caixa estar identificando corretamente um objeto. Além disso, deve-se criar uma lista com as probabilidades de todas as caixas das imagens.\n",
    "\n",
    "O formato dessa lista de probabilidades é o mesmo usado para definir as classes de cada caixa. Contudo, as probabilidades são números reais variando de 0.0 a 1.0. Como essas probabilidades são somente para permitir visualizar as imagens de treinamento com as caixas deve-se definir todas as probabilidades iguais a 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para você fazer: Defir lista de probabilidades das caixas\n",
    "\n",
    "# Inicializa lista de probabilidades\n",
    "dummy_scores=[]\n",
    "\n",
    "# Define vetor de probabilidade das caixas de cada imagem e une a lista, segundo o exemplos\n",
    "# score1 = [1., 1.]\n",
    "# dummy_scores.append(score1)\n",
    "# Insira seu código aqui\n",
    "#\n",
    "\n",
    "# Apresenta resultados\n",
    "print(dummy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para visualizar as imagens de treinamento com as caixas sobrepostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dmq-OfNOZNjz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 60))\n",
    "for idx, train_image_np in enumerate(train_images_np):\n",
    "    plt.subplot(2, 3, idx+1)\n",
    "    plot_detections(train_images_np[idx], gt_boxes[idx], classes[idx], dummy_scores[idx], category_index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Da7rxNKZNjz"
   },
   "source": [
    "## 4. Criar modelo pré-treinado e restaurar seus parâmetros \n",
    "\n",
    "Nesta seção deve-se construir o modelo de detecção de objetos do tipo RetinaNet e carregar os seus parâmetros pré-treinados. \n",
    "\n",
    "Nesse trabalho recomenda-se usar o mesmo modelo usado da aula de detecção de objetos com a API do tensorFlow, ou seja, o modelo SSD-ResNet50 para imagens com dimensão 640x640x3. Esse modelo utiliza o método SSD para detecção dos objejos, usando a rede ResNet-50 para extrair as características das imagens. Se quiser pode tentar usar outro modelo, mas cuidado porque você pode ter dificuldade para configurá-lo e criá-lo.\n",
    "\n",
    "Execute a célula abaixo para carregar os parâmetros do modelo e os colocá-los na pasta do Colab \"content/research/models/detection/test_data\".\n",
    "\n",
    "Observa-se que se você escolheu usar outro modelo é necessário alterar o código das duas células que realizam essas operações de criar o modelo e carregar seus parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0VeFSBeZNjz"
   },
   "outputs": [],
   "source": [
    "# Carrega o arquivo de parâmetros (\"checkpoint\") e o coloca na pasta \n",
    "# models/research/object_detection/test_data/SSD_ResNet_50\n",
    "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n77QwcKsVsj0"
   },
   "source": [
    "Para criar o modelo é usado o arquivo de configuração, que define a arquitetura do modelo, e os parâmetros pré-treinados (\"checkpoints\").\n",
    "\n",
    "Ao executarmos a célula acima carregamos o arquivo de configuração e os parâmetros pré-treinados do modelo que iremos usar. \n",
    "\n",
    "- Arquivo de configuração: \"models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
    "- Prefixo dos arquivos com parâmetros pré-treinados do modelo: \"models/research/object_detection/test_data/checkpoint/ckpt-0\"\n",
    "\n",
    "Se abrir o arquivo de configuração é possível ver a configuração do modelo.\n",
    "\n",
    "<br>\n",
    "\n",
    "Execute a  célula abaixo para criar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uwb5carVZNj0"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "print('Construindo modelo e carregando parâmetros pré-treinados...', flush=True)\n",
    "\n",
    "# Define número de classes do novo conjunto de dados\n",
    "num_classes = 2\n",
    "\n",
    "# Define arquivos de configuraçao e prefixo dos arquivos de parâmetros\n",
    "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# Define diretório para slavar arquivo de configuração e parãmetros do modelo retreinado\n",
    "output_directory = 'output/'\n",
    "output_checkpoint_dir = os.path.join(output_directory, 'checkpoint')\n",
    "\n",
    "# Alteração do arquivo de configuração \n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "\n",
    "# Construção do modelo completo com parâmetros inicializados aleatóriamente\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=True)\n",
    "\n",
    "# Salva novo arquivo de configuração \n",
    "pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_proto, output_directory)\n",
    "\n",
    "# Cria subrede de identificação de caixas usando a subrede do modelo pré-treinado\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    # Se for desejado deixar também a subrede de classificação, deve-se tirar o comentário da linha abaixo\n",
    "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "\n",
    "# Cria modelo parcial com rede de extração de caracteríticas e subrede de identificação das caixas \n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "             _feature_extractor=detection_model._feature_extractor,\n",
    "             _box_predictor=fake_box_predictor)\n",
    "\n",
    "# Carrega parâmetros pré-treinados no modelo parcial, ou seja, na rede de extração \n",
    "# de característica e na subrede de identificação das caixas\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "# Transfere modelo parcial com parâmetros pré-treinados para modelo final\n",
    "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(exported_ckpt, output_checkpoint_dir, max_to_keep=1)\n",
    "\n",
    "# Executa modelo com imagem dummy para inicializar variáveis criadas\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "print('Modelo criado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVoy8ETOVsj1"
   },
   "source": [
    "Mostra novo arquivo de configuração para inspeção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6Tj4_RDd3v3"
   },
   "outputs": [],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3VsFuzcZNj0"
   },
   "source": [
    "## 5. Treinamento do modelo\n",
    "\n",
    "A célula abaixo define os parâmetros do modelo que será retreinado e executa o treinamento. \n",
    "\n",
    "\n",
    "### Exercício #7: Seleção dos parâmetros de treinamento\n",
    "\n",
    "Na ceúla abaixo você deve definir os seguintes parâmetros de treinamento:\n",
    "\n",
    "- `batch_size` - define tamanho do lote;\n",
    "- `learning_rate` - define taxa de aprendizado;\n",
    "- `num_batches` - define quantos lotes são utilizados no treinamento;\n",
    "- Otimizador a ser usado.\n",
    "\n",
    "Observe que o lopp de treinamento se repete em função do número de lotes. Por exemplo, se existirem 100 exemplos de treinamento e os lotes (`batch_size`) forem de 25 exemplos, tem-se 4 lotes de exemplos. Assim, se o número de lotes (`num_batches`) for igual a 200, então cada lote de dados será utlizado no treinamemto 50 vezes.\n",
    "\n",
    "Após definir esses parâmetros basta executar a célula para realizar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NC9pPDZZNj0"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Seleção de parâmetros de treinamemto\n",
    "\n",
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# Define tamanho do lote de treinamento\n",
    "batch_size = \n",
    "\n",
    "# Define taxa de treinamento\n",
    "learning_rate =\n",
    "\n",
    "# Define número de lotes usados no treinamento\n",
    "num_batches = \n",
    "\n",
    "\n",
    "# Define parâmetros do modelo que são alterados no treinamento\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "        to_fine_tune.append(var)\n",
    "\n",
    "        \n",
    "# Define função para carregar modelo e otimizador para treinamento\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "    \"\"\"Get a tf.function for training step.\"\"\"\n",
    "\n",
    "    # Define função para executar um passo de treinamento que processa um lote de dados\n",
    "    # Função é criada na forma tf.function para aumentar velocidade de computação\n",
    "    # Pode-se comentar a linha @tf.function para desabilitar essa opção\n",
    "    #@tf.function\n",
    "    def train_step_fn(image_tensors,\n",
    "                      groundtruth_boxes_list,\n",
    "                      groundtruth_classes_list):\n",
    "        \"\"\"A single training iteration.\n",
    "\n",
    "        Args:\n",
    "        image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "             Note that the height and width can vary across images, as they are\n",
    "             reshaped within this function to be 640x640.\n",
    "        groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "             tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "        groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "             with type tf.float32 representing groundtruth boxes for each image in\n",
    "             the batch.\n",
    "\n",
    "        Returns:\n",
    "            A scalar tensor representing the total loss for the input batch.\n",
    "        \"\"\"\n",
    "        # Define dimensão do tensor com dados e entrada\n",
    "        shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
    "        \n",
    "        # Obtém saídas reais \n",
    "        model.provide_groundtruth(\n",
    "              groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "              groundtruth_classes_list=groundtruth_classes_list)\n",
    "        \n",
    "        # Calcula gradiente da função de custo em relação aos parâmetros\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Pré-processa imagens do lote\n",
    "            preprocessed_images = tf.concat([detection_model.preprocess(image_tensor)[0]\n",
    "                                             for image_tensor in image_tensors], axis=0)\n",
    "            \n",
    "            # Executa modelo para obter previsões das classes e caixas\n",
    "            prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "            \n",
    "            # Calcula função de custo\n",
    "            losses_dict = model.loss(prediction_dict, shapes)\n",
    "            total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "            \n",
    "            # Calcula gradiente da função de custo em relação aos parâmetros treináveis\n",
    "            gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "            \n",
    "            # Aplica otimizador para atualizar parãmetros\n",
    "            optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "            \n",
    "        return total_loss\n",
    "    return train_step_fn\n",
    "\n",
    "# Define otimizador (SGD com momento) \n",
    "# Insira seu código aqui\n",
    "# optimizer = \n",
    "\n",
    "# Inicializa função de treinamento passando modelo, otimizador e lista de parâmetros treináveis\n",
    "train_step_fn = get_model_train_step_function(detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "print('Inicio do treinamento!', flush=True)\n",
    "\n",
    "# Executa loop de treinamento \n",
    "for idx in range(num_batches):\n",
    "    # Embaralha dados aleatóriamente\n",
    "    all_keys = list(range(len(train_images_np)))\n",
    "    random.shuffle(all_keys)\n",
    "    example_keys = all_keys[:batch_size]\n",
    "\n",
    "    # Obtém dados de treinamenteo \n",
    "    gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
    "    gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
    "    image_tensors = [train_image_tensors[key] for key in example_keys]\n",
    "\n",
    "    # Realiza um passo de treinamento\n",
    "    total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print('batch ' + str(idx) + ' of ' + str(num_batches) + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
    "\n",
    "# Salva novos parâmetros do modelo         \n",
    "ckpt_manager.save()\n",
    "\n",
    "print('Término do treinamento!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEk9UHs4ZNj1"
   },
   "source": [
    "## 7. teste do modelo\n",
    "\n",
    "Para testar o modelo retreinado com o novo conjunto de dados vamos usar as imagens de teste.\n",
    "\n",
    "\n",
    "### Exercício #8: Gerar lista de arquivos das imagens de teste\n",
    "\n",
    "Para carregar as imagens de treinamento deve-se gerar uma lista com os arquivos presentes na pasta \"cangurus_pessoas/test\". Crie um código na célula abaixo para gerar uma lista dos arquivos das imagens de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSAbMQrEZNj2"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Criar lista de arquivos das imagens de teste\n",
    "\n",
    "# Define diretório onde se encontram as imagens\n",
    "test_dir = \n",
    "\n",
    "# Escolhe tipos de arquivos desejados\n",
    "glob_imgs = \n",
    "\n",
    "# Cria lista dos nomes dos arquivos\n",
    "img_paths = \n",
    "\n",
    "# Número de imagens de teste\n",
    "print(len(img_paths))\n",
    "\n",
    "# Inicializa lista de imagens de teste\n",
    "test_images_np = []\n",
    "\n",
    "# Le arquivos da imagens e inclui dados na lista de imagens\n",
    "# Insira seu código aqui\n",
    "#\n",
    "\n",
    "\n",
    "# Mostra imagens de teste\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['xtick.labelsize'] = False\n",
    "plt.rcParams['ytick.labelsize'] = False\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['figure.figsize'] = [16, 60]\n",
    "\n",
    "for idx, test_image_np in enumerate(test_images_np):\n",
    "    plt.subplot(6, 5, idx+1)\n",
    "    plt.imshow(test_image_np)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para criar a a função `detect()`, que recebe uma imagem na forma de tensor TF e calcula as previsões de objetos presentes na imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria função para calcular previsões do modelo\n",
    "# Função é criada na forma tf.function para aumentar velocidade de computação\n",
    "# Pode-se comentar a linha @tf.function para desabilitar essa opção\n",
    "@tf.function\n",
    "def detect(input_tensor):\n",
    "    \"\"\"Run detection on an input image.\n",
    "\n",
    "    Args:\n",
    "        input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "        Note that height and width can be anything since the image will be\n",
    "        immediately resized according to the needs of the model within this\n",
    "        function.\n",
    "\n",
    "    Returns:\n",
    "       A dict containing 3 Tensors (`detection_boxes`, `detection_classes`, and `detection_scores`).\n",
    "    \"\"\"\n",
    "    # Pré-processa imagens\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    # calcula previsões do modelo\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    # Retorna previsões pós-processadas\n",
    "    return detection_model.postprocess(prediction_dict, shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #9: Calculo das pervisões do modelo\n",
    "\n",
    "Complete a célula abaixo e depois a execute para calcular as previsões do seu modelo e mostrar as imagens com as caixas detectadas sobrepostas.\n",
    "\n",
    "Note que a saída da função `detect()` é um dicionário que contém as seguintes `keys`:\n",
    "\n",
    "    detections['detection_boxes'];\n",
    "    detections['detection_classes'];\n",
    "    detections['detection_scores'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPgtYRDUpxPD"
   },
   "outputs": [],
   "source": [
    "# Para você fazer: Preparação das imagens de cálculo e cálculo das previsões\n",
    "\n",
    "# Tamanho de apresentação das imagens \n",
    "plt.figure(figsize=(15, 60))\n",
    "\n",
    "# Loop para executar previsões do modelo nas imagens de teste\n",
    "for i in range(len(test_images_np)):\n",
    "\n",
    "    # Transforma imagens em tensor do TF de números reais\n",
    "    input_tensor = \n",
    "    \n",
    "    # Inclui eixo dos exemplos nas imagens\n",
    "    input_tensor = )\n",
    "    \n",
    "    # Calcula previsões\n",
    "    detections = \n",
    "\n",
    "    # Chama função plot_detections() para criar imagens com  as caixas\n",
    "    plt.subplot(9, 3, i+1)\n",
    "    # Insira seu código aqui\n",
    "    #\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHNfjYBrhxCC"
   },
   "source": [
    "## 8. Utilização do modelo treinado\n",
    "\n",
    "Como visto em aula, o modelo criado utiliza camadas customizadas e o método de programação em \"graphos\" do TensorFlow de forma que para ser utilizado após ter sido salvo ele tem que ser reconstruído. O modelo salvo é reconstruído a partir do arquivo de configuração e dos parâmetros salvos. Assim, se quiser reutilizar esse modelo você deve seguir os passos apresentados em aula para fazer isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "T2_Detecao_objeto_TF_sol_1.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
